{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Matplotlib forms basis for visualization in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We will use the Seaborn library\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Graphics in SVG format are more sharp and legible\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4          5        6   \\\n",
      "0       -9.4674  0.137210  1.054500  0.238620 -0.033490  -0.188480  -9.8425   \n",
      "1       -9.5991  0.154810  1.182400  0.226060 -0.079540  -0.362630  -9.7832   \n",
      "2       -9.8612  0.133170  1.064600  0.205130 -0.096285  -0.248230  -9.8010   \n",
      "3       -9.6398  0.174060  1.236700  0.184200 -0.062794  -0.250270  -9.6730   \n",
      "4      -10.0360  0.260620  1.276100  0.125590 -0.025118  -0.206680  -9.9102   \n",
      "5       -9.6700  0.167460  0.857180  0.062794 -0.033490  -0.390890  -9.9108   \n",
      "6       -9.2655  0.650970  1.395400  0.025118 -0.041863  -0.105080  -9.8407   \n",
      "7       -9.6621  0.277490  0.972340  0.012559 -0.037677  -0.495030  -9.8329   \n",
      "8       -9.4799  0.257660  1.117900  0.020931 -0.025118  -0.106760  -9.7624   \n",
      "9       -9.6593  0.134870  1.096300  0.025118 -0.020931  -0.065413  -9.8618   \n",
      "10      -9.6210  0.243950  1.337700  0.025118 -0.025118  -0.126850  -9.8621   \n",
      "11      -9.6293  0.154090  1.224400  0.012559 -0.041863  -0.206950  -9.6405   \n",
      "12      -9.5744 -0.067700  1.236000  0.008373 -0.041863  -0.350770  -9.7815   \n",
      "13      -9.1642  0.128990  1.169500 -0.004186 -0.054422  -0.247470  -9.8802   \n",
      "14      -9.3535 -0.004205  1.168000 -0.016745 -0.071167  -0.236710  -9.8296   \n",
      "15      -9.7328  0.276890  0.961760 -0.004186 -0.041863  -0.217500  -9.9109   \n",
      "16      -9.4566  0.105040  1.251600 -0.016745 -0.075353  -0.277700  -9.7898   \n",
      "17      -9.7576  0.001639  1.189600 -0.016745 -0.054422  -0.491340  -9.9692   \n",
      "18      -9.7179  0.034731  0.950490  0.000000 -0.041863  -0.339570  -9.9305   \n",
      "19      -9.6393  0.145370  1.076200  0.033490 -0.033490  -0.361800  -9.7723   \n",
      "20      -9.7245  0.365380  1.211800  0.050235 -0.016745  -0.370720  -9.6110   \n",
      "21      -9.2213 -0.045356  1.374900 -0.050235 -0.079540  -0.145490  -9.8403   \n",
      "22      -9.6786  0.091257  1.397700 -0.046049 -0.108840  -0.265440 -10.0280   \n",
      "23      -9.6177  0.074837  1.080400 -0.066981 -0.129770  -0.247900 -10.0110   \n",
      "24      -9.4476  0.157590  1.056300 -0.071167 -0.108840  -0.307170  -9.8987   \n",
      "25      -9.8208  0.131680  1.254000 -0.154890 -0.138150  -0.627350  -9.6725   \n",
      "26      -9.7406  0.163280  1.204200  0.552590  0.113030  -0.278450  -9.7106   \n",
      "27      -9.7744 -0.170620  1.226700  3.127200  1.754100  -0.150970 -10.0860   \n",
      "28      -9.7482  0.034533  0.939950 -0.330720 -0.125590  -0.272920  -9.9850   \n",
      "29      -9.3760  0.115200  1.326200 -0.125590 -0.092098  -0.276830  -9.7489   \n",
      "...         ...       ...       ...       ...       ...        ...      ...   \n",
      "122082  -5.4386 -0.289920 -0.689250 -1.276800  0.117220   0.157380  -4.8143   \n",
      "122083  -6.0157  0.278710  0.456660 -1.281000  0.163270   1.343100  -4.5025   \n",
      "122084  -6.5886  0.135800  0.033275 -1.222400  0.259550   0.982270  -5.5763   \n",
      "122085  -8.2397  0.369920  0.222770 -1.151200  0.376770   1.228000  -6.4956   \n",
      "122086  -8.1721  0.525760 -0.142160 -1.138700  0.422820   0.953100  -8.0845   \n",
      "122087  -7.9731  0.676390  0.145090 -1.142900  0.452120   0.666870  -9.1848   \n",
      "122088  -7.7674  0.996660 -0.362930 -1.109400  0.506540   0.845210 -11.8000   \n",
      "122089  -8.6483  1.100800  0.467490 -1.121900  0.523290   0.497940 -14.7400   \n",
      "122090  -9.1212  0.504180  1.066400 -1.205700  0.514910   2.216400 -17.8250   \n",
      "122091  -9.3978  0.194360  0.459600 -1.247500  0.531660   1.613100 -19.3070   \n",
      "122092 -10.2760  0.680190  0.492800 -1.268400  0.556780  -7.450600 -19.4110   \n",
      "122093 -10.7320  1.749800  0.146960 -1.272600  0.615380  -5.695400 -19.4630   \n",
      "122094 -11.3070  1.192600 -0.261090 -1.235000  0.632130  -2.294900 -12.8520   \n",
      "122095 -12.8110  0.646040 -0.647120 -1.193100  0.661430  -8.659200  -5.7505   \n",
      "122096 -13.9410 -0.431310 -1.112500 -1.343800  0.594450  19.466000 -12.1960   \n",
      "122097 -14.2530 -2.004900 -2.654100 -1.406600  0.560960  -4.443200 -13.4060   \n",
      "122098 -13.1350 -0.802020 -4.651100 -1.440100  0.556780 -19.515000 -19.2370   \n",
      "122099  -9.7995  1.086900 -3.333800 -1.578200  0.527470  -2.672600 -19.0800   \n",
      "122100  -8.3786  0.241870 -3.273000  1.858700  2.495000  -4.984000 -17.8020   \n",
      "122101  -8.0139  0.697340 -4.010900 -1.858700  0.355830   4.089800  -8.3598   \n",
      "122102  -9.3850  0.055839 -2.847600 -1.595000  0.527470  -2.843000  -9.4345   \n",
      "122103 -11.5580 -0.364930 -3.348600 -1.607500  0.527470   4.569800 -10.7800   \n",
      "122104 -10.5230 -1.103900 -3.162500 -1.582400  0.527470   3.222600 -10.7600   \n",
      "122105  -9.9442 -0.264810 -2.655400 -1.548900  0.560960   1.747300 -10.7260   \n",
      "122106  -9.1037 -0.372050 -2.201900 -1.431700  0.636320   1.775800 -10.2880   \n",
      "122107  -7.9261  0.344050 -1.926900 -1.385700  0.715860   1.539200  -9.7096   \n",
      "122108  -7.3952  0.064886 -1.762600 -1.348000  0.720040   2.072100  -9.7372   \n",
      "122109  -7.7164 -0.543520 -1.848900 -1.385700  0.673990   2.058600  -9.7005   \n",
      "122110  -9.4886 -1.326700 -2.066800 -1.322900  0.644690   2.249000  -9.7241   \n",
      "122111  -9.9039 -1.773400 -2.264900 -1.281000  0.703300   1.748200  -9.3951   \n",
      "\n",
      "               7        8        9   ...        14       15        16  \\\n",
      "0        0.901830  0.55659 -0.83490  ...  -9.08970   2.0431  1.310500   \n",
      "1        0.892630  0.55659 -0.83490  ...  -9.60100   1.8242  1.424500   \n",
      "2        1.090800  0.57885 -0.84615  ...  -8.74460   2.4401  1.007400   \n",
      "3        1.013800  0.57885 -0.84615  ...  -9.89470   1.9019  1.188800   \n",
      "4        1.054500  0.57885 -0.84615  ...  -9.65220   2.0140  1.402800   \n",
      "5        1.000600  0.55102 -0.83865  ...  -9.59630   2.3111  1.105100   \n",
      "6        1.079800  0.55102 -0.83865  ...  -9.80590   2.4704  1.040200   \n",
      "7        0.867990  0.55102 -0.83865  ...  -9.69360   2.4597  0.966730   \n",
      "8        0.989980  0.55102 -0.83865  ...  -9.37930   2.2788  0.884780   \n",
      "9        0.943750  0.56772 -0.83114  ... -10.03800   2.0725  1.249500   \n",
      "10       0.922290  0.56772 -0.83114  ...  -9.64160   2.0404  1.039500   \n",
      "11       1.297200  0.56772 -0.83114  ...  -9.57920   1.9997  0.974990   \n",
      "12       1.059500  0.56401 -0.84615  ...  -9.59630   1.8559  0.592800   \n",
      "13       1.086800  0.56401 -0.84615  ...  -9.31670   1.8893  0.927800   \n",
      "14       1.193700  0.56401 -0.84615  ...  -8.94270   1.7584  0.837920   \n",
      "15       0.991860  0.56401 -0.84615  ...  -9.36110   1.6482  0.821990   \n",
      "16       1.216100  0.56586 -0.82927  ...  -9.48160   1.5786  0.860180   \n",
      "17       1.100800  0.56586 -0.82927  ...  -9.58230   1.4194  0.935550   \n",
      "18       1.010500  0.56586 -0.82927  ...  -9.22810   1.6040  0.403120   \n",
      "19       0.986930  0.56957 -0.82552  ...  -9.29930   1.4881  0.813990   \n",
      "20       1.276200  0.56957 -0.82552  ...  -9.20330   1.3998  0.976240   \n",
      "21       1.121100  0.56957 -0.82552  ... -10.09800   1.5640  1.401100   \n",
      "22       1.183500  0.56586 -0.84053  ...  -9.37340   1.7088  0.877110   \n",
      "23       0.913650  0.56586 -0.84053  ...  -9.60830   1.7802  1.016100   \n",
      "24       1.220700  0.56586 -0.84053  ...  -9.31480   1.6395  0.946560   \n",
      "25       1.062200  0.56586 -0.84053  ...  -9.34000   1.4684  0.844260   \n",
      "26       1.220000  0.56586 -0.83490  ...  -9.36600   1.4206  1.060600   \n",
      "27       1.348400  0.56586 -0.83490  ...  -9.41040   1.3423  1.233800   \n",
      "28       1.498000  0.56586 -0.83490  ...  -9.59590   1.4638  1.375300   \n",
      "29       1.343100  0.55102 -0.84240  ...  -9.48610   1.7030  1.303900   \n",
      "...           ...      ...      ...  ...       ...      ...       ...   \n",
      "122082   1.740000  0.50464 -0.16698  ...  -3.03010 -18.6310 -0.119060   \n",
      "122083   1.236200  0.50464 -0.16698  ...  -3.17940 -18.5880  0.174750   \n",
      "122084  -0.206490  0.53061 -0.24015  ...  -2.48370 -17.5340 -0.366270   \n",
      "122085  -1.041700  0.53061 -0.24015  ...  -3.16010 -16.5380 -0.816490   \n",
      "122086  -0.530110  0.53061 -0.24015  ...  -3.13550 -15.5230 -0.307400   \n",
      "122087  -0.660910  0.54917 -0.40525  ...  -3.20290 -14.4510 -0.087127   \n",
      "122088  -1.769000  0.54917 -0.40525  ...  -3.59220 -13.3380 -0.812740   \n",
      "122089  -2.700800  0.54917 -0.40525  ...  -3.94230 -11.6440 -1.373900   \n",
      "122090  -3.327400  0.54917 -0.40525  ...  -4.72500 -10.5960 -1.634000   \n",
      "122091  -6.980500  0.51948 -0.62101  ...  -4.59850  -9.5354 -1.539300   \n",
      "122092 -10.389000  0.51948 -0.62101  ...  -5.22180  -8.6772 -0.715110   \n",
      "122093 -16.551000  0.51948 -0.62101  ...  -5.32960  -7.9736 -0.357010   \n",
      "122094 -18.874000  0.50093 -0.69981  ...  -3.77850  -7.6550  0.502670   \n",
      "122095 -18.614000  0.50093 -0.69981  ...  -2.69570  -6.9250  0.497500   \n",
      "122096 -18.644000  0.50093 -0.69981  ...  -3.22650  -5.8450  0.499420   \n",
      "122097 -12.829000  0.50093 -0.69981  ...  -4.16360  -5.4241  0.593380   \n",
      "122098  10.905000  0.37662 -0.71295  ...  -3.77590  -4.6525  1.746500   \n",
      "122099  18.759000  0.37662 -0.71295  ...  -3.78730  -4.3963  3.373900   \n",
      "122100  -2.939900  0.37662 -0.71295  ...  -3.17620  -4.6823  3.765900   \n",
      "122101   1.643700  0.44527 -0.64728  ...  -4.10770  -4.9542  4.575200   \n",
      "122102  -0.875150  0.44527 -0.64728  ...  -4.37710  -5.0342  4.581600   \n",
      "122103  -1.764100  0.44527 -0.64728  ...  -4.64310  -4.6800  5.995200   \n",
      "122104  -0.745350  0.39518 -0.58724  ...  -3.30460  -4.6264  7.362400   \n",
      "122105  -0.279220  0.39518 -0.58724  ...  -1.43630  -4.5627  7.730200   \n",
      "122106  -0.059128  0.39518 -0.58724  ...  -1.86910  -5.3259  7.410800   \n",
      "122107   0.332000  0.39518 -0.58724  ...  -3.46300  -6.7532  7.683200   \n",
      "122108   0.533860  0.37477 -0.57036  ...   0.41986  -6.6179  7.210800   \n",
      "122109   0.243890  0.37477 -0.57036  ...   0.60193  -6.7714  7.859300   \n",
      "122110  -0.130380  0.37477 -0.57036  ...   1.29320  -6.4462  8.375600   \n",
      "122111  -0.900190  0.34323 -0.57786  ...   1.12920  -6.3951  9.492700   \n",
      "\n",
      "              17       18        19          20        21          22  23  \n",
      "0      -0.754900 -0.22998  0.782330   -1.605100   1.06850   -3.652900   0  \n",
      "1      -0.754900 -0.22998  0.782330   -2.101500   5.29040    0.627090   0  \n",
      "2      -0.754900 -0.22998  0.782330    0.565620   2.70210   -1.459800   0  \n",
      "3      -0.754900 -0.22998  0.782330    0.244770   6.52330   -0.782360   0  \n",
      "4      -0.752940 -0.22998  0.762930    0.034368   3.43680   -1.116700   0  \n",
      "5      -0.752940 -0.22998  0.762930    0.215810   3.62730   -0.031957   0  \n",
      "6      -0.752940 -0.22998  0.762930   -1.368900   6.73500    0.627290   0  \n",
      "7      -0.770590 -0.22793  0.745690   -4.433000   5.53090    4.188200   0  \n",
      "8      -0.770590 -0.22793  0.745690   -5.660700   8.43920    3.412500   0  \n",
      "9      -0.770590 -0.22793  0.745690   -6.928200   7.36280    3.397900   0  \n",
      "10     -0.770590 -0.22793  0.745690   -6.076200   2.78880    0.572610   0  \n",
      "11     -0.766670 -0.24435  0.734910   -6.049100   5.50700    0.184960   0  \n",
      "12     -0.766670 -0.24435  0.734910   -7.115400   6.59180   -1.291300   0  \n",
      "13     -0.766670 -0.24435  0.734910   -7.470900   6.94740   -2.385000   0  \n",
      "14     -0.788240 -0.24025  0.711210   -8.002400   7.65680   -4.568700   0  \n",
      "15     -0.788240 -0.24025  0.711210   -8.651600  14.54800   -5.733700   0  \n",
      "16     -0.788240 -0.24025  0.711210   -9.078000   7.81640   -7.841000   0  \n",
      "17     -0.788240 -0.24025  0.711210   -8.044600   3.43650   -9.220500   0  \n",
      "18     -0.786270 -0.27926  0.711210   -7.328500   3.24080   -9.926300   0  \n",
      "19     -0.786270 -0.27926  0.711210   -6.461600   0.15815   -8.795100   0  \n",
      "20     -0.786270 -0.27926  0.711210   -5.560600   0.49783  -10.224000   0  \n",
      "21     -0.780390 -0.29363  0.696120   -3.218900   1.26570   -3.687100   0  \n",
      "22     -0.780390 -0.29363  0.696120   -1.438300  -0.19986   -3.275900   0  \n",
      "23     -0.780390 -0.29363  0.696120    0.505740  -3.28590   -1.401100   0  \n",
      "24     -0.780390 -0.29363  0.696120    2.103500  -5.09460    0.815040   0  \n",
      "25     -0.784310 -0.30390  0.704740    3.713800  -5.64030    2.296700   0  \n",
      "26     -0.784310 -0.30390  0.704740    4.067500  -6.18100    3.031200   0  \n",
      "27     -0.784310 -0.30390  0.704740    5.326000  -6.00100    4.137600   0  \n",
      "28     -0.776470 -0.28542  0.700430    5.873700  -5.08820    5.222500   0  \n",
      "29     -0.776470 -0.28542  0.700430    6.584600  -5.81740    5.605000   0  \n",
      "...          ...      ...       ...         ...       ...         ...  ..  \n",
      "122082  0.892160 -0.70637  0.017241   99.231000 -58.98600  226.040000   0  \n",
      "122083  0.892160 -0.70637  0.017241  103.690000 -61.63300  238.070000   0  \n",
      "122084  0.678430 -0.92402  0.075431  106.780000 -57.73300  250.010000   0  \n",
      "122085  0.678430 -0.92402  0.075431  107.560000 -51.63300  261.870000   0  \n",
      "122086  0.678430 -0.92402  0.075431  105.240000 -50.47100  266.870000   0  \n",
      "122087  0.678430 -0.92402  0.075431  103.100000 -48.99800  266.810000   0  \n",
      "122088  0.400000 -1.06160  0.239220   99.586000 -41.37800  263.060000   0  \n",
      "122089  0.400000 -1.06160  0.239220   90.843000 -35.94600  253.080000   0  \n",
      "122090  0.400000 -1.06160  0.239220   81.130000 -37.81700  237.740000   0  \n",
      "122091  0.147060 -1.08830  0.420260   71.230000 -40.41600  222.050000   0  \n",
      "122092  0.147060 -1.08830  0.420260   62.102000 -37.57400  206.670000   0  \n",
      "122093  0.147060 -1.08830  0.420260   55.792000 -40.18700  193.210000   0  \n",
      "122094  0.147060 -1.08830  0.420260   51.818000 -42.64400  179.440000   0  \n",
      "122095 -0.058824 -1.05130  0.594830   49.918000 -35.11900  167.780000   0  \n",
      "122096 -0.058824 -1.05130  0.594830   45.195000 -22.68100  154.560000   0  \n",
      "122097 -0.058824 -1.05130  0.594830   35.854000 -23.26100  141.740000   0  \n",
      "122098 -0.166670 -0.99589  0.719830   24.318000 -27.83200  126.760000   0  \n",
      "122099 -0.166670 -0.99589  0.719830   14.006000 -35.68600  111.470000   0  \n",
      "122100 -0.166670 -0.99589  0.719830    5.360200 -38.48600   95.077000   0  \n",
      "122101 -0.166670 -0.99589  0.719830   -4.349500 -39.98000   81.179000   0  \n",
      "122102 -0.201960 -0.97536  0.799570  -16.250000 -45.10700   64.747000   0  \n",
      "122103 -0.201960 -0.97536  0.799570  -26.258000 -58.49500   58.905000   0  \n",
      "122104 -0.201960 -0.97536  0.799570  -26.767000 -73.43700   70.596000   0  \n",
      "122105 -0.133330 -1.01440  0.788790  -27.565000 -81.45800   84.379000   0  \n",
      "122106 -0.133330 -1.01440  0.788790  -31.801000 -74.15800  102.270000   0  \n",
      "122107 -0.133330 -1.01440  0.788790  -31.169000 -82.74900  115.010000   0  \n",
      "122108 -0.133330 -1.01440  0.788790  -27.081000 -86.96300  115.130000   0  \n",
      "122109 -0.105880 -1.07390  0.767240  -25.230000 -63.37900  116.010000   0  \n",
      "122110 -0.105880 -1.07390  0.767240  -20.491000 -38.34600  119.470000   0  \n",
      "122111 -0.105880 -1.07390  0.767240  -13.590000 -12.45700  120.070000   0  \n",
      "\n",
      "[122112 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mHealth_subject3.log\", header=None, delim_whitespace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the samples size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples of 0 activity is (86732, 24)\n",
      "number of samples of 1 activity is (3072, 24)\n",
      "number of samples of 2 activity is (3072, 24)\n",
      "number of samples of 3 activity is (3072, 24)\n",
      "number of samples of 4 activity is (3072, 24)\n",
      "number of samples of 5 activity is (3072, 24)\n",
      "number of samples of 6 activity is (3226, 24)\n",
      "number of samples of 7 activity is (3379, 24)\n",
      "number of samples of 8 activity is (3175, 24)\n",
      "number of samples of 9 activity is (3072, 24)\n",
      "number of samples of 10 activity is (3072, 24)\n",
      "number of samples of 11 activity is (3072, 24)\n",
      "number of samples of 12 activity is (1024, 24)\n"
     ]
    }
   ],
   "source": [
    "for i in range(13): \n",
    "    df1=df[df[23] == i]\n",
    "    print('number of samples of', i,\"activity is\", df1.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we found it better to exclude the labels with different size to end up with 8 classes rether than 12   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df[23] != 6]\n",
    "df=df[df[23] != 7]\n",
    "df=df[df[23] != 8]\n",
    "df=df[df[23] != 12]\n",
    "df=df[df[23] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract ECG column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractECG(dataFrame,lead=1):\n",
    "    if lead == 1:\n",
    "        lead = 4\n",
    "    else :\n",
    "        lead = 5\n",
    "        \n",
    "    data = dataFrame[lead]\n",
    "#     data = data.reset_index()\n",
    "#     data.columns=[\"index\",\"readings\"]\n",
    "    return data\n",
    "data=extractECG(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine labels based on frequency\n",
    "for example for one seconds classification every 50 readings belongs to one label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "readings=25\n",
    "def fmax(label_count):\n",
    "    M=0\n",
    "    for i in range (int (len (label_count)) ):\n",
    "        if label_count[i]>label_count[M]:\n",
    "            M=i\n",
    "    return M\n",
    "labels=[]\n",
    "label_index=0\n",
    "label = df[23].values\n",
    "label_count=[0,0,0,0,0,0,0,0]\n",
    "for i in range(int(len(label))):\n",
    "    if label[i]==1:\n",
    "        label_count[0]=label_count[0]+1\n",
    "    elif label[i]==2:\n",
    "        label_count[1]=label_count[1]+1\n",
    "    elif label[i]==3:\n",
    "        label_count[2]=label_count[2]+1\n",
    "    elif label[i]==4:\n",
    "        label_count[3]=label_count[3]+1\n",
    "    elif label[i]==5:\n",
    "        label_count[4]=label_count[4]+1\n",
    "    elif label[i]==9:\n",
    "        label_count[5]=label_count[5]+1\n",
    "    elif label[i]==10:\n",
    "        label_count[6]=label_count[6]+1\n",
    "    elif label[i]==11:\n",
    "        label_count[7]=label_count[7]+1\n",
    "    \n",
    "    if(i%readings==readings-1):\n",
    "        M=fmax(label_count)\n",
    "        labels.append(M)\n",
    "        label_count=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        label_index=label_index+1\n",
    "labels=np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "based on some research we found that there are many features that can be measured from the ecg signal. we choose 2 features that are possible to be implemented.\n",
    "* number of peaks in the signal\n",
    "* heartrate[1] \n",
    "\n",
    "We will see if these 2 features are useful for prediction or not.\n",
    "\n",
    "The following 2 blocks contain functions for features extraction\n",
    "\n",
    "The heartrate is not measured to be beat/min. Instead it is dependent on number of readings we specify. As we know the data has a frequency of 50 readings/sec so when we use 25 readings the heartrate unit will be beat/halfsecond and so on    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmax(ecg):\n",
    "    \n",
    "    M=0\n",
    "    \n",
    "    index=0 \n",
    "    for i in range(int(len(ecg))):\n",
    "        if M < ecg[i]:\n",
    "            M=ecg[i]\n",
    "            index=i\n",
    "        \n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def findmaxindex(ecg,peaks):\n",
    "    \n",
    "    M=0\n",
    "    \n",
    "    index=0\n",
    "    for i in range(int(len(peaks))):\n",
    "        if M < ecg[peaks[i]]:\n",
    "            M=ecg[peaks[i]]\n",
    "            index=i\n",
    "        \n",
    "    \n",
    "    return index\n",
    "\n",
    "def findpeak(ecg):\n",
    "    \n",
    "    start=0\n",
    "    end=10\n",
    "    \n",
    "    peaks=[]\n",
    "  \n",
    "    for i in range(int(len(ecg)/10)):\n",
    "        ecg1=ecg[start:end]\n",
    "        ecg1=ecg1.reset_index(drop=True)\n",
    "        peaks.append(findmax(ecg1))\n",
    "        start=start+10\n",
    "        end=end+10\n",
    "        \n",
    "    return peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks1=findpeaksper2second(data[100:200])\n",
    "# print(peaks1)\n",
    "\n",
    "\n",
    "\n",
    "heartrate=[]\n",
    "numbofpeaks=[]\n",
    "\n",
    "def arrange(peaks):\n",
    "    for i in range(int(len(peaks))):\n",
    "        if peaks[i]!=-1 and peaks[i]!=0:\n",
    "            peaks[i]=i*10+peaks[i]\n",
    "            \n",
    "    return peaks\n",
    "    \n",
    "for i in range (int(len(data)/readings)):\n",
    "    data1=data[i*readings:(i+1)*readings]\n",
    "    data1=data1.reset_index(drop=True)\n",
    "    \n",
    "    peaks1=findpeak(data1)\n",
    "#     print (peaks1)\n",
    "    maxindex=findmaxindex(data1,peaks1)\n",
    "        \n",
    "#     print(peaks1)    \n",
    "    for j in range (int(len(peaks1))):\n",
    "            \n",
    "        if data1[peaks1[maxindex]]-data1[peaks1[j]]>0.7 :\n",
    "            peaks1[j]=-1\n",
    "#     print(peaks1)\n",
    "    \n",
    "    n=0\n",
    "    for j in range (int(len(peaks1))):\n",
    "        if peaks1[j]!=-1 or peaks1[j]!=0:\n",
    "            n=n+1\n",
    "    numbofpeaks.append(n)\n",
    "    peaks1=arrange(peaks1)\n",
    "#     print(peaks1)\n",
    "#     break\n",
    "    firstpeak=0\n",
    "    secondpeak=0\n",
    "    for j in range (int(len(peaks1))):\n",
    "        if peaks1[j]!=-1 or peaks1[j]!=0:\n",
    "            if firstpeak==0:\n",
    "                firstpeak=peaks1[j]\n",
    "            elif secondpeak==0:\n",
    "                secondpeak=peaks1[j]\n",
    "            else :\n",
    "                break\n",
    "    hr=abs(((firstpeak-secondpeak))/readings)\n",
    "    heartrate.append(hr)\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the ECG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.values\n",
    "labels1=df[23].values\n",
    "for j in range(data.size%readings):\n",
    "    data = np.delete(data, (j), axis=0)\n",
    "data= data.reshape(int(data.size/readings),1,readings)\n",
    "numbofpeaks=np.array(numbofpeaks)\n",
    "numbofpeaks= numbofpeaks.reshape(int(numbofpeaks.size),1,1)\n",
    "heartrate=np.array(heartrate)\n",
    "heartrate= heartrate.reshape(int(heartrate.size),1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 types of data that can be used for models   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spliting the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since it is a time series data it would be better to split the data by taking the first big part for training and the last small part for validation and testing. However, this can't be done because the last part of the data doesn't have the labels that the first part has. so different classes might appear in the testing set. Therefore, we used split function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(heartrate, to_categorical(labels), test_size = 0.20, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heartrate 1D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yaseen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/yaseen/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1, 64)             128       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 41,672\n",
      "Trainable params: 41,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(786, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "model.summary()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yaseen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "786/786 [==============================] - 2s 2ms/step - loss: 2.0813 - acc: 0.1310\n",
      "Epoch 2/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 2.0758 - acc: 0.1336\n",
      "Epoch 3/100\n",
      "786/786 [==============================] - 0s 49us/step - loss: 2.0724 - acc: 0.1641\n",
      "Epoch 4/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 2.0685 - acc: 0.1578\n",
      "Epoch 5/100\n",
      "786/786 [==============================] - 0s 58us/step - loss: 2.0631 - acc: 0.1489\n",
      "Epoch 6/100\n",
      "786/786 [==============================] - 0s 52us/step - loss: 2.0581 - acc: 0.1438\n",
      "Epoch 7/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 2.0516 - acc: 0.1298\n",
      "Epoch 8/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 2.0424 - acc: 0.1654\n",
      "Epoch 9/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 2.0443 - acc: 0.1616\n",
      "Epoch 10/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 2.0403 - acc: 0.1590\n",
      "Epoch 11/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 2.0414 - acc: 0.1705\n",
      "Epoch 12/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 2.0372 - acc: 0.1718\n",
      "Epoch 13/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 2.0301 - acc: 0.1807\n",
      "Epoch 14/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 2.0274 - acc: 0.1756\n",
      "Epoch 15/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 2.0267 - acc: 0.1527\n",
      "Epoch 16/100\n",
      "786/786 [==============================] - 0s 48us/step - loss: 2.0334 - acc: 0.1565\n",
      "Epoch 17/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 2.0260 - acc: 0.1641\n",
      "Epoch 18/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 2.0244 - acc: 0.1603\n",
      "Epoch 19/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 2.0204 - acc: 0.1527\n",
      "Epoch 20/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 2.0169 - acc: 0.1578\n",
      "Epoch 21/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 2.0155 - acc: 0.1616\n",
      "Epoch 22/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 2.0105 - acc: 0.1705\n",
      "Epoch 23/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 2.0113 - acc: 0.1870\n",
      "Epoch 24/100\n",
      "786/786 [==============================] - 0s 70us/step - loss: 2.0128 - acc: 0.1692\n",
      "Epoch 25/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 2.0168 - acc: 0.1781\n",
      "Epoch 26/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 2.0163 - acc: 0.1654\n",
      "Epoch 27/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 2.0134 - acc: 0.1819\n",
      "Epoch 28/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 2.0137 - acc: 0.1807\n",
      "Epoch 29/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 2.0073 - acc: 0.1743\n",
      "Epoch 30/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 2.0105 - acc: 0.1819\n",
      "Epoch 31/100\n",
      "786/786 [==============================] - 0s 52us/step - loss: 2.0078 - acc: 0.1641\n",
      "Epoch 32/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 2.0102 - acc: 0.1858\n",
      "Epoch 33/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 2.0024 - acc: 0.1756\n",
      "Epoch 34/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 2.0064 - acc: 0.1819\n",
      "Epoch 35/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 2.0119 - acc: 0.1768\n",
      "Epoch 36/100\n",
      "786/786 [==============================] - 0s 70us/step - loss: 2.0038 - acc: 0.1819\n",
      "Epoch 37/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 2.0037 - acc: 0.1807\n",
      "Epoch 38/100\n",
      "786/786 [==============================] - 0s 50us/step - loss: 2.0071 - acc: 0.1718\n",
      "Epoch 39/100\n",
      "786/786 [==============================] - 0s 50us/step - loss: 2.0061 - acc: 0.1819\n",
      "Epoch 40/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 2.0025 - acc: 0.1908\n",
      "Epoch 41/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 2.0011 - acc: 0.1883\n",
      "Epoch 42/100\n",
      "786/786 [==============================] - 0s 51us/step - loss: 2.0131 - acc: 0.1845\n",
      "Epoch 43/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 2.0094 - acc: 0.1641\n",
      "Epoch 44/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 2.0061 - acc: 0.1819\n",
      "Epoch 45/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 2.0059 - acc: 0.1705\n",
      "Epoch 46/100\n",
      "786/786 [==============================] - 0s 69us/step - loss: 2.0079 - acc: 0.1832\n",
      "Epoch 47/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 2.0113 - acc: 0.1807\n",
      "Epoch 48/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 2.0055 - acc: 0.1921\n",
      "Epoch 49/100\n",
      "786/786 [==============================] - 0s 66us/step - loss: 2.0038 - acc: 0.1858\n",
      "Epoch 50/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 2.0019 - acc: 0.1934\n",
      "Epoch 51/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 2.0058 - acc: 0.1794\n",
      "Epoch 52/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 1.9979 - acc: 0.1858\n",
      "Epoch 53/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 2.0059 - acc: 0.1832\n",
      "Epoch 54/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 2.0053 - acc: 0.1908\n",
      "Epoch 55/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 2.0015 - acc: 0.1781\n",
      "Epoch 56/100\n",
      "786/786 [==============================] - 0s 74us/step - loss: 2.0011 - acc: 0.1870\n",
      "Epoch 57/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 2.0054 - acc: 0.1858\n",
      "Epoch 58/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.9984 - acc: 0.1832\n",
      "Epoch 59/100\n",
      "786/786 [==============================] - 0s 94us/step - loss: 2.0022 - acc: 0.1832\n",
      "Epoch 60/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 2.0024 - acc: 0.1578\n",
      "Epoch 61/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 2.0007 - acc: 0.1858\n",
      "Epoch 62/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 1.9952 - acc: 0.1921\n",
      "Epoch 63/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 2.0033 - acc: 0.1870\n",
      "Epoch 64/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 1.9943 - acc: 0.1921\n",
      "Epoch 65/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.9958 - acc: 0.1908\n",
      "Epoch 66/100\n",
      "786/786 [==============================] - 0s 74us/step - loss: 2.0012 - acc: 0.1997\n",
      "Epoch 67/100\n",
      "786/786 [==============================] - 0s 75us/step - loss: 1.9947 - acc: 0.1934\n",
      "Epoch 68/100\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.9917 - acc: 0.1908\n",
      "Epoch 69/100\n",
      "786/786 [==============================] - 0s 71us/step - loss: 2.0007 - acc: 0.1807\n",
      "Epoch 70/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.9983 - acc: 0.1947\n",
      "Epoch 71/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 2.0035 - acc: 0.1794\n",
      "Epoch 72/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 1.9977 - acc: 0.1896\n",
      "Epoch 73/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 1.9962 - acc: 0.1845\n",
      "Epoch 74/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.9990 - acc: 0.1870\n",
      "Epoch 75/100\n",
      "786/786 [==============================] - 0s 66us/step - loss: 2.0002 - acc: 0.1819\n",
      "Epoch 76/100\n",
      "786/786 [==============================] - 0s 66us/step - loss: 2.0008 - acc: 0.1870\n",
      "Epoch 77/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 1.9968 - acc: 0.1997\n",
      "Epoch 78/100\n",
      "786/786 [==============================] - 0s 52us/step - loss: 1.9960 - acc: 0.1845\n",
      "Epoch 79/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 2.0013 - acc: 0.1794\n",
      "Epoch 80/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 1.9994 - acc: 0.1896\n",
      "Epoch 81/100\n",
      "786/786 [==============================] - 0s 47us/step - loss: 2.0037 - acc: 0.1718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 1.9992 - acc: 0.1972\n",
      "Epoch 83/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 2.0035 - acc: 0.1819\n",
      "Epoch 84/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 1.9976 - acc: 0.1819\n",
      "Epoch 85/100\n",
      "786/786 [==============================] - 0s 72us/step - loss: 1.9961 - acc: 0.1883\n",
      "Epoch 86/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 1.9865 - acc: 0.1959\n",
      "Epoch 87/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 1.9984 - acc: 0.2010\n",
      "Epoch 88/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 1.9977 - acc: 0.1870\n",
      "Epoch 89/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 1.9971 - acc: 0.1870\n",
      "Epoch 90/100\n",
      "786/786 [==============================] - 0s 49us/step - loss: 2.0026 - acc: 0.1858\n",
      "Epoch 91/100\n",
      "786/786 [==============================] - 0s 51us/step - loss: 1.9993 - acc: 0.1934\n",
      "Epoch 92/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 1.9938 - acc: 0.1858\n",
      "Epoch 93/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 1.9925 - acc: 0.1832\n",
      "Epoch 94/100\n",
      "786/786 [==============================] - 0s 52us/step - loss: 1.9943 - acc: 0.1947\n",
      "Epoch 95/100\n",
      "786/786 [==============================] - 0s 51us/step - loss: 1.9907 - acc: 0.1768\n",
      "Epoch 96/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 1.9968 - acc: 0.1870\n",
      "Epoch 97/100\n",
      "786/786 [==============================] - 0s 52us/step - loss: 1.9922 - acc: 0.1883\n",
      "Epoch 98/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 1.9979 - acc: 0.1883\n",
      "Epoch 99/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 1.9912 - acc: 0.1730\n",
      "Epoch 100/100\n",
      "786/786 [==============================] - 0s 78us/step - loss: 1.9943 - acc: 0.1870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce6e4e0748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train = X_train.reshape((392, 1, 50))\n",
    "\n",
    "# fit network\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "# evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 0s 264us/step\n",
      "[2.009075570227531, 0.1319796954314721]\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of peaks model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 1, 1)\n",
      "(786, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 1, 64)             128       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 41,672\n",
      "Trainable params: 41,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "786/786 [==============================] - 2s 3ms/step - loss: 2.0839 - acc: 0.1361\n",
      "Epoch 2/100\n",
      "786/786 [==============================] - 0s 252us/step - loss: 2.0847 - acc: 0.1272\n",
      "Epoch 3/100\n",
      "786/786 [==============================] - 0s 235us/step - loss: 2.0856 - acc: 0.1069\n",
      "Epoch 4/100\n",
      "786/786 [==============================] - 0s 235us/step - loss: 2.0825 - acc: 0.1234\n",
      "Epoch 5/100\n",
      "786/786 [==============================] - 0s 210us/step - loss: 2.0807 - acc: 0.1170\n",
      "Epoch 6/100\n",
      "786/786 [==============================] - 0s 250us/step - loss: 2.0803 - acc: 0.1272\n",
      "Epoch 7/100\n",
      "786/786 [==============================] - 0s 458us/step - loss: 2.0797 - acc: 0.1412\n",
      "Epoch 8/100\n",
      "786/786 [==============================] - 0s 367us/step - loss: 2.0836 - acc: 0.1120\n",
      "Epoch 9/100\n",
      "786/786 [==============================] - 0s 317us/step - loss: 2.0841 - acc: 0.1132\n",
      "Epoch 10/100\n",
      "786/786 [==============================] - 0s 585us/step - loss: 2.0785 - acc: 0.1438\n",
      "Epoch 11/100\n",
      "786/786 [==============================] - 0s 497us/step - loss: 2.0810 - acc: 0.1145\n",
      "Epoch 12/100\n",
      "786/786 [==============================] - 0s 290us/step - loss: 2.0803 - acc: 0.1476\n",
      "Epoch 13/100\n",
      "786/786 [==============================] - 0s 254us/step - loss: 2.0791 - acc: 0.1247\n",
      "Epoch 14/100\n",
      "786/786 [==============================] - 0s 448us/step - loss: 2.0812 - acc: 0.1260\n",
      "Epoch 15/100\n",
      "786/786 [==============================] - 0s 334us/step - loss: 2.0776 - acc: 0.1349 0s - loss: 2.0753 - acc: 0.13\n",
      "Epoch 16/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 2.0799 - acc: 0.1336\n",
      "Epoch 17/100\n",
      "786/786 [==============================] - 0s 340us/step - loss: 2.0804 - acc: 0.1374\n",
      "Epoch 18/100\n",
      "786/786 [==============================] - 0s 570us/step - loss: 2.0806 - acc: 0.1374\n",
      "Epoch 19/100\n",
      "786/786 [==============================] - 0s 366us/step - loss: 2.0790 - acc: 0.1374\n",
      "Epoch 20/100\n",
      "786/786 [==============================] - 0s 434us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 21/100\n",
      "786/786 [==============================] - 0s 336us/step - loss: 2.0790 - acc: 0.1374\n",
      "Epoch 22/100\n",
      "786/786 [==============================] - 0s 273us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 23/100\n",
      "786/786 [==============================] - 0s 256us/step - loss: 2.0786 - acc: 0.1374\n",
      "Epoch 24/100\n",
      "786/786 [==============================] - 0s 334us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 25/100\n",
      "786/786 [==============================] - 0s 567us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 26/100\n",
      "786/786 [==============================] - 0s 353us/step - loss: 2.0786 - acc: 0.1374\n",
      "Epoch 27/100\n",
      "786/786 [==============================] - 0s 420us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 28/100\n",
      "786/786 [==============================] - 0s 340us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 29/100\n",
      "786/786 [==============================] - 0s 562us/step - loss: 2.0779 - acc: 0.1374 0s - loss: 2.0803 - acc: 0.1\n",
      "Epoch 30/100\n",
      "786/786 [==============================] - 0s 469us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 31/100\n",
      "786/786 [==============================] - 0s 366us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 32/100\n",
      "786/786 [==============================] - 0s 577us/step - loss: 2.0785 - acc: 0.1374\n",
      "Epoch 33/100\n",
      "786/786 [==============================] - 0s 312us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 34/100\n",
      "786/786 [==============================] - 0s 270us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 35/100\n",
      "786/786 [==============================] - 0s 502us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 36/100\n",
      "786/786 [==============================] - 0s 483us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 37/100\n",
      "786/786 [==============================] - 0s 359us/step - loss: 2.0784 - acc: 0.1374\n",
      "Epoch 38/100\n",
      "786/786 [==============================] - 0s 378us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 39/100\n",
      "786/786 [==============================] - 0s 507us/step - loss: 2.0777 - acc: 0.1374\n",
      "Epoch 40/100\n",
      "786/786 [==============================] - 0s 331us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 41/100\n",
      "786/786 [==============================] - 0s 388us/step - loss: 2.0786 - acc: 0.1374\n",
      "Epoch 42/100\n",
      "786/786 [==============================] - 0s 482us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 43/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 44/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 45/100\n",
      "786/786 [==============================] - 0s 526us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 46/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 47/100\n",
      "786/786 [==============================] - 0s 322us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 48/100\n",
      "786/786 [==============================] - 0s 315us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 49/100\n",
      "786/786 [==============================] - 0s 518us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 50/100\n",
      "786/786 [==============================] - 0s 284us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 51/100\n",
      "786/786 [==============================] - 0s 308us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 52/100\n",
      "786/786 [==============================] - 0s 276us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 53/100\n",
      "786/786 [==============================] - 0s 521us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 54/100\n",
      "786/786 [==============================] - 0s 356us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 55/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 56/100\n",
      "786/786 [==============================] - 0s 278us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 57/100\n",
      "786/786 [==============================] - 0s 425us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 58/100\n",
      "786/786 [==============================] - 0s 327us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 59/100\n",
      "786/786 [==============================] - 0s 472us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 60/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 61/100\n",
      "786/786 [==============================] - 0s 300us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 62/100\n",
      "786/786 [==============================] - 0s 253us/step - loss: 2.0781 - acc: 0.1374 0s - loss: 2.0768 - acc: 0.1\n",
      "Epoch 63/100\n",
      "786/786 [==============================] - 0s 364us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 64/100\n",
      "786/786 [==============================] - 0s 605us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 65/100\n",
      "786/786 [==============================] - 0s 357us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 66/100\n",
      "786/786 [==============================] - 0s 300us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 67/100\n",
      "786/786 [==============================] - 0s 343us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 68/100\n",
      "786/786 [==============================] - 0s 588us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 69/100\n",
      "786/786 [==============================] - 0s 257us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786/786 [==============================] - 0s 295us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 71/100\n",
      "786/786 [==============================] - 0s 397us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 72/100\n",
      "786/786 [==============================] - 0s 413us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 73/100\n",
      "786/786 [==============================] - 0s 329us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 74/100\n",
      "786/786 [==============================] - 0s 277us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 75/100\n",
      "786/786 [==============================] - 0s 488us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 76/100\n",
      "786/786 [==============================] - 0s 417us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 77/100\n",
      "786/786 [==============================] - 0s 284us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 78/100\n",
      "786/786 [==============================] - 0s 200us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 79/100\n",
      "786/786 [==============================] - 0s 576us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 80/100\n",
      "786/786 [==============================] - 0s 385us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 81/100\n",
      "786/786 [==============================] - 0s 365us/step - loss: 2.0780 - acc: 0.1374 0s - loss: 2.0799 - acc: 0.12\n",
      "Epoch 82/100\n",
      "786/786 [==============================] - 0s 429us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 83/100\n",
      "786/786 [==============================] - 0s 315us/step - loss: 2.0784 - acc: 0.1374\n",
      "Epoch 84/100\n",
      "786/786 [==============================] - 0s 347us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 85/100\n",
      "786/786 [==============================] - 0s 295us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 86/100\n",
      "786/786 [==============================] - 0s 262us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 87/100\n",
      "786/786 [==============================] - 0s 456us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 88/100\n",
      "786/786 [==============================] - 0s 317us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 89/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 2.0780 - acc: 0.1374 0s - loss: 2.0768 - acc: 0.14\n",
      "Epoch 90/100\n",
      "786/786 [==============================] - 0s 305us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 91/100\n",
      "786/786 [==============================] - 0s 536us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 92/100\n",
      "786/786 [==============================] - 0s 353us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 93/100\n",
      "786/786 [==============================] - 0s 320us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 94/100\n",
      "786/786 [==============================] - 0s 450us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 95/100\n",
      "786/786 [==============================] - 0s 338us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 96/100\n",
      "786/786 [==============================] - 0s 345us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 97/100\n",
      "786/786 [==============================] - 0s 444us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 98/100\n",
      "786/786 [==============================] - 0s 388us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 99/100\n",
      "786/786 [==============================] - 0s 404us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 100/100\n",
      "786/786 [==============================] - 0s 341us/step - loss: 2.0780 - acc: 0.1374\n",
      "197/197 [==============================] - 1s 3ms/step\n",
      "[2.095928105000917, 0.07614213197969544]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(numbofpeaks, to_categorical(labels), test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model_numofpeaks = Sequential()\n",
    "model_numofpeaks.add(Conv1D(filters=64, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model_numofpeaks.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model_numofpeaks.add(Dropout(0.5))\n",
    "model_numofpeaks.add(MaxPooling1D(pool_size=1))\n",
    "model_numofpeaks.add(Flatten())\n",
    "model_numofpeaks.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model_numofpeaks.add(Dense(n_outputs, activation='softmax'))\n",
    "model_numofpeaks.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_numofpeaks.summary()\n",
    "model_numofpeaks.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "accuracy = model_numofpeaks.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 1, 25)\n",
      "(786, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 1, 64)             1664      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 43,208\n",
      "Trainable params: 43,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "786/786 [==============================] - 0s 396us/step - loss: 2.0308 - acc: 0.1399\n",
      "Epoch 2/100\n",
      "786/786 [==============================] - 0s 51us/step - loss: 1.9223 - acc: 0.1845\n",
      "Epoch 3/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 1.8370 - acc: 0.2417\n",
      "Epoch 4/100\n",
      "786/786 [==============================] - 0s 50us/step - loss: 1.7670 - acc: 0.3346\n",
      "Epoch 5/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.6787 - acc: 0.3588\n",
      "Epoch 6/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 1.6061 - acc: 0.3626\n",
      "Epoch 7/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 1.5248 - acc: 0.4326\n",
      "Epoch 8/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 1.4996 - acc: 0.4173\n",
      "Epoch 9/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 1.4288 - acc: 0.4478\n",
      "Epoch 10/100\n",
      "786/786 [==============================] - 0s 78us/step - loss: 1.3974 - acc: 0.4529\n",
      "Epoch 11/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 1.3637 - acc: 0.4746\n",
      "Epoch 12/100\n",
      "786/786 [==============================] - 0s 68us/step - loss: 1.3054 - acc: 0.4809\n",
      "Epoch 13/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.2984 - acc: 0.4898\n",
      "Epoch 14/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.2523 - acc: 0.5280\n",
      "Epoch 15/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 1.2218 - acc: 0.5293\n",
      "Epoch 16/100\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.2095 - acc: 0.5471\n",
      "Epoch 17/100\n",
      "786/786 [==============================] - 0s 51us/step - loss: 1.2008 - acc: 0.5140\n",
      "Epoch 18/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 1.1614 - acc: 0.5522\n",
      "Epoch 19/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 1.1556 - acc: 0.5573\n",
      "Epoch 20/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 1.0885 - acc: 0.5751\n",
      "Epoch 21/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 1.1118 - acc: 0.5547\n",
      "Epoch 22/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 1.0840 - acc: 0.5789\n",
      "Epoch 23/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 1.0427 - acc: 0.5992\n",
      "Epoch 24/100\n",
      "786/786 [==============================] - 0s 58us/step - loss: 1.1087 - acc: 0.5649\n",
      "Epoch 25/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.0509 - acc: 0.5751\n",
      "Epoch 26/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 1.0444 - acc: 0.5992\n",
      "Epoch 27/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.9633 - acc: 0.6298\n",
      "Epoch 28/100\n",
      "786/786 [==============================] - 0s 66us/step - loss: 0.9912 - acc: 0.6209\n",
      "Epoch 29/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.9944 - acc: 0.6196\n",
      "Epoch 30/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.9586 - acc: 0.6387\n",
      "Epoch 31/100\n",
      "786/786 [==============================] - 0s 68us/step - loss: 0.9422 - acc: 0.6399\n",
      "Epoch 32/100\n",
      "786/786 [==============================] - 0s 58us/step - loss: 0.9458 - acc: 0.6450\n",
      "Epoch 33/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.9357 - acc: 0.6374\n",
      "Epoch 34/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 0.9047 - acc: 0.6590\n",
      "Epoch 35/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.8943 - acc: 0.6628\n",
      "Epoch 36/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 0.8767 - acc: 0.6743\n",
      "Epoch 37/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.8521 - acc: 0.6692\n",
      "Epoch 38/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.8260 - acc: 0.6972\n",
      "Epoch 39/100\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.8428 - acc: 0.6807\n",
      "Epoch 40/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 0.7963 - acc: 0.7125\n",
      "Epoch 41/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.8242 - acc: 0.6743\n",
      "Epoch 42/100\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.8025 - acc: 0.6921\n",
      "Epoch 43/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 0.7781 - acc: 0.7112\n",
      "Epoch 44/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 0.8114 - acc: 0.6832\n",
      "Epoch 45/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.8243 - acc: 0.6845\n",
      "Epoch 46/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.7692 - acc: 0.7214\n",
      "Epoch 47/100\n",
      "786/786 [==============================] - 0s 80us/step - loss: 0.7501 - acc: 0.7125\n",
      "Epoch 48/100\n",
      "786/786 [==============================] - 0s 84us/step - loss: 0.7651 - acc: 0.6908\n",
      "Epoch 49/100\n",
      "786/786 [==============================] - 0s 79us/step - loss: 0.7573 - acc: 0.7074\n",
      "Epoch 50/100\n",
      "786/786 [==============================] - 0s 72us/step - loss: 0.7270 - acc: 0.7379\n",
      "Epoch 51/100\n",
      "786/786 [==============================] - 0s 68us/step - loss: 0.7551 - acc: 0.7214\n",
      "Epoch 52/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 0.7164 - acc: 0.7417\n",
      "Epoch 53/100\n",
      "786/786 [==============================] - 0s 80us/step - loss: 0.6896 - acc: 0.7468\n",
      "Epoch 54/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 0.6905 - acc: 0.7341\n",
      "Epoch 55/100\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.6780 - acc: 0.7570\n",
      "Epoch 56/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 0.6898 - acc: 0.7379\n",
      "Epoch 57/100\n",
      "786/786 [==============================] - 0s 79us/step - loss: 0.6686 - acc: 0.7430\n",
      "Epoch 58/100\n",
      "786/786 [==============================] - 0s 66us/step - loss: 0.6542 - acc: 0.7595\n",
      "Epoch 59/100\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.6527 - acc: 0.7532\n",
      "Epoch 60/100\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.6524 - acc: 0.7621\n",
      "Epoch 61/100\n",
      "786/786 [==============================] - 0s 84us/step - loss: 0.6615 - acc: 0.7532\n",
      "Epoch 62/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 0.6054 - acc: 0.7748\n",
      "Epoch 63/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 0.6092 - acc: 0.7710\n",
      "Epoch 64/100\n",
      "786/786 [==============================] - 0s 84us/step - loss: 0.6130 - acc: 0.7774\n",
      "Epoch 65/100\n",
      "786/786 [==============================] - 0s 78us/step - loss: 0.5897 - acc: 0.7888\n",
      "Epoch 66/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 0.5887 - acc: 0.7799\n",
      "Epoch 67/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.5962 - acc: 0.7672\n",
      "Epoch 68/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 0.5627 - acc: 0.7977\n",
      "Epoch 69/100\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.5412 - acc: 0.8041\n",
      "Epoch 70/100\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.5637 - acc: 0.7901\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786/786 [==============================] - 0s 59us/step - loss: 0.5347 - acc: 0.8079\n",
      "Epoch 72/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 0.5110 - acc: 0.8104\n",
      "Epoch 73/100\n",
      "786/786 [==============================] - 0s 57us/step - loss: 0.5636 - acc: 0.8003\n",
      "Epoch 74/100\n",
      "786/786 [==============================] - 0s 58us/step - loss: 0.5306 - acc: 0.7952\n",
      "Epoch 75/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 0.5370 - acc: 0.8028\n",
      "Epoch 76/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 0.5287 - acc: 0.7952\n",
      "Epoch 77/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 0.5165 - acc: 0.7964\n",
      "Epoch 78/100\n",
      "786/786 [==============================] - 0s 58us/step - loss: 0.5363 - acc: 0.7990\n",
      "Epoch 79/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 0.5021 - acc: 0.8193\n",
      "Epoch 80/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 0.4922 - acc: 0.8155\n",
      "Epoch 81/100\n",
      "786/786 [==============================] - 0s 56us/step - loss: 0.5178 - acc: 0.7888\n",
      "Epoch 82/100\n",
      "786/786 [==============================] - 0s 48us/step - loss: 0.5025 - acc: 0.8206\n",
      "Epoch 83/100\n",
      "786/786 [==============================] - 0s 47us/step - loss: 0.4561 - acc: 0.8244\n",
      "Epoch 84/100\n",
      "786/786 [==============================] - 0s 52us/step - loss: 0.4760 - acc: 0.8232\n",
      "Epoch 85/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 0.4912 - acc: 0.8257\n",
      "Epoch 86/100\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.4589 - acc: 0.8219\n",
      "Epoch 87/100\n",
      "786/786 [==============================] - 0s 59us/step - loss: 0.4879 - acc: 0.8130\n",
      "Epoch 88/100\n",
      "786/786 [==============================] - 0s 47us/step - loss: 0.4498 - acc: 0.8359\n",
      "Epoch 89/100\n",
      "786/786 [==============================] - 0s 47us/step - loss: 0.5014 - acc: 0.8244\n",
      "Epoch 90/100\n",
      "786/786 [==============================] - 0s 53us/step - loss: 0.4687 - acc: 0.8321\n",
      "Epoch 91/100\n",
      "786/786 [==============================] - 0s 51us/step - loss: 0.4635 - acc: 0.8499\n",
      "Epoch 92/100\n",
      "786/786 [==============================] - 0s 47us/step - loss: 0.4407 - acc: 0.8435\n",
      "Epoch 93/100\n",
      "786/786 [==============================] - 0s 51us/step - loss: 0.4723 - acc: 0.8270\n",
      "Epoch 94/100\n",
      "786/786 [==============================] - 0s 55us/step - loss: 0.4160 - acc: 0.8486\n",
      "Epoch 95/100\n",
      "786/786 [==============================] - 0s 73us/step - loss: 0.4637 - acc: 0.8270\n",
      "Epoch 96/100\n",
      "786/786 [==============================] - 0s 67us/step - loss: 0.4048 - acc: 0.8562\n",
      "Epoch 97/100\n",
      "786/786 [==============================] - 0s 54us/step - loss: 0.4428 - acc: 0.8410\n",
      "Epoch 98/100\n",
      "786/786 [==============================] - 0s 80us/step - loss: 0.4087 - acc: 0.8410\n",
      "Epoch 99/100\n",
      "786/786 [==============================] - 0s 68us/step - loss: 0.4041 - acc: 0.8486\n",
      "Epoch 100/100\n",
      "786/786 [==============================] - 0s 69us/step - loss: 0.3859 - acc: 0.8613\n",
      "197/197 [==============================] - 0s 432us/step\n",
      "[1.7911124277841015, 0.5380710661411285]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, to_categorical(labels), test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model_data = Sequential()\n",
    "model_data.add(Conv1D(filters=64, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model_data.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model_data.add(Dropout(0.5))\n",
    "model_data.add(MaxPooling1D(pool_size=1))\n",
    "model_data.add(Flatten())\n",
    "model_data.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model_data.add(Dense(n_outputs, activation='softmax'))\n",
    "model_data.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_data.summary()\n",
    "model_data.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "accuracy = model_data.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another try with scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def scale_data(trainX, testX, standardize):\n",
    "\t# remove overlap\n",
    "\tcut = int(trainX.shape[1] / 2)\n",
    "\tlongX = trainX[:, -cut:, :]\n",
    "\t# flatten windows\n",
    "\tlongX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
    "\t# flatten train and test\n",
    "\tflatTrainX = trainX.reshape((trainX.shape[0] * trainX.shape[1], trainX.shape[2]))\n",
    "\tflatTestX = testX.reshape((testX.shape[0] * testX.shape[1], testX.shape[2]))\n",
    "\t# standardize\n",
    "\tif standardize:\n",
    "\t\ts = StandardScaler()\n",
    "\t\t# fit on training data\n",
    "\t\ts.fit(longX)\n",
    "\t\t# apply to training and test data\n",
    "\t\tlongX = s.transform(longX)\n",
    "\t\tflatTrainX = s.transform(flatTrainX)\n",
    "\t\tflatTestX = s.transform(flatTestX)\n",
    "\t# reshape\n",
    "\tflatTrainX = flatTrainX.reshape((trainX.shape))\n",
    "\tflatTestX = flatTestX.reshape((testX.shape))\n",
    "\treturn flatTrainX, flatTestX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX = scale_data(X_train, X_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model1 = Sequential()\n",
    "model1.add(Conv1D(filters=128, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(MaxPooling1D(pool_size=1))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "# model1.add(Dense(64, activation='relu'))\n",
    "# model1.add(BatchNormalization())\n",
    "model1.add(Dense(n_outputs, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 1, 128)            3328      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 64)             256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 17,288\n",
      "Trainable params: 16,776\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "786/786 [==============================] - 1s 1ms/step - loss: 2.5101 - acc: 0.1578\n",
      "Epoch 2/150\n",
      "786/786 [==============================] - 0s 69us/step - loss: 2.0295 - acc: 0.2341\n",
      "Epoch 3/150\n",
      "786/786 [==============================] - 0s 71us/step - loss: 1.9192 - acc: 0.2748\n",
      "Epoch 4/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.7491 - acc: 0.3168\n",
      "Epoch 5/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.6972 - acc: 0.3333\n",
      "Epoch 6/150\n",
      "786/786 [==============================] - 0s 68us/step - loss: 1.6568 - acc: 0.3511\n",
      "Epoch 7/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.6146 - acc: 0.3842\n",
      "Epoch 8/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 1.5663 - acc: 0.3919\n",
      "Epoch 9/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 1.5200 - acc: 0.3982\n",
      "Epoch 10/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 1.5300 - acc: 0.3957\n",
      "Epoch 11/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 1.4995 - acc: 0.4173\n",
      "Epoch 12/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 1.4683 - acc: 0.4211\n",
      "Epoch 13/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.4468 - acc: 0.4427\n",
      "Epoch 14/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 1.4246 - acc: 0.4211\n",
      "Epoch 15/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 1.4434 - acc: 0.4275\n",
      "Epoch 16/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.3961 - acc: 0.4746\n",
      "Epoch 17/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 1.4257 - acc: 0.4173\n",
      "Epoch 18/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.3564 - acc: 0.4771\n",
      "Epoch 19/150\n",
      "786/786 [==============================] - 0s 69us/step - loss: 1.3668 - acc: 0.4555\n",
      "Epoch 20/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.3647 - acc: 0.4542\n",
      "Epoch 21/150\n",
      "786/786 [==============================] - 0s 77us/step - loss: 1.3058 - acc: 0.4746\n",
      "Epoch 22/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.3402 - acc: 0.4911\n",
      "Epoch 23/150\n",
      "786/786 [==============================] - 0s 71us/step - loss: 1.2687 - acc: 0.5038\n",
      "Epoch 24/150\n",
      "786/786 [==============================] - 0s 85us/step - loss: 1.2758 - acc: 0.5051\n",
      "Epoch 25/150\n",
      "786/786 [==============================] - 0s 78us/step - loss: 1.3452 - acc: 0.4809\n",
      "Epoch 26/150\n",
      "786/786 [==============================] - 0s 84us/step - loss: 1.2500 - acc: 0.5216\n",
      "Epoch 27/150\n",
      "786/786 [==============================] - 0s 86us/step - loss: 1.2562 - acc: 0.5267\n",
      "Epoch 28/150\n",
      "786/786 [==============================] - 0s 87us/step - loss: 1.2367 - acc: 0.4885\n",
      "Epoch 29/150\n",
      "786/786 [==============================] - 0s 80us/step - loss: 1.1932 - acc: 0.5433\n",
      "Epoch 30/150\n",
      "786/786 [==============================] - 0s 75us/step - loss: 1.2285 - acc: 0.5254\n",
      "Epoch 31/150\n",
      "786/786 [==============================] - 0s 84us/step - loss: 1.2431 - acc: 0.5038\n",
      "Epoch 32/150\n",
      "786/786 [==============================] - 0s 79us/step - loss: 1.2119 - acc: 0.4987\n",
      "Epoch 33/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.1794 - acc: 0.5420\n",
      "Epoch 34/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 1.1783 - acc: 0.5407\n",
      "Epoch 35/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 1.1871 - acc: 0.5369\n",
      "Epoch 36/150\n",
      "786/786 [==============================] - 0s 70us/step - loss: 1.1213 - acc: 0.5687\n",
      "Epoch 37/150\n",
      "786/786 [==============================] - 0s 72us/step - loss: 1.1285 - acc: 0.5585\n",
      "Epoch 38/150\n",
      "786/786 [==============================] - 0s 77us/step - loss: 1.1420 - acc: 0.5598\n",
      "Epoch 39/150\n",
      "786/786 [==============================] - 0s 93us/step - loss: 1.1296 - acc: 0.5471\n",
      "Epoch 40/150\n",
      "786/786 [==============================] - 0s 85us/step - loss: 1.1207 - acc: 0.5636\n",
      "Epoch 41/150\n",
      "786/786 [==============================] - 0s 109us/step - loss: 1.0833 - acc: 0.5712\n",
      "Epoch 42/150\n",
      "786/786 [==============================] - 0s 101us/step - loss: 1.0810 - acc: 0.5878\n",
      "Epoch 43/150\n",
      "786/786 [==============================] - 0s 102us/step - loss: 1.0150 - acc: 0.6170\n",
      "Epoch 44/150\n",
      "786/786 [==============================] - 0s 91us/step - loss: 1.0429 - acc: 0.5916\n",
      "Epoch 45/150\n",
      "786/786 [==============================] - 0s 93us/step - loss: 1.0426 - acc: 0.6018\n",
      "Epoch 46/150\n",
      "786/786 [==============================] - 0s 89us/step - loss: 1.0149 - acc: 0.5992\n",
      "Epoch 47/150\n",
      "786/786 [==============================] - 0s 79us/step - loss: 1.0374 - acc: 0.6005\n",
      "Epoch 48/150\n",
      "786/786 [==============================] - 0s 68us/step - loss: 1.0403 - acc: 0.6018\n",
      "Epoch 49/150\n",
      "786/786 [==============================] - 0s 68us/step - loss: 1.0786 - acc: 0.5547\n",
      "Epoch 50/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 0.9995 - acc: 0.6132\n",
      "Epoch 51/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.0250 - acc: 0.6056\n",
      "Epoch 52/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.0313 - acc: 0.6170\n",
      "Epoch 53/150\n",
      "786/786 [==============================] - 0s 70us/step - loss: 0.9664 - acc: 0.6247\n",
      "Epoch 54/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 1.0322 - acc: 0.6018\n",
      "Epoch 55/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.9640 - acc: 0.6234\n",
      "Epoch 56/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.9843 - acc: 0.6018\n",
      "Epoch 57/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.9543 - acc: 0.6145\n",
      "Epoch 58/150\n",
      "786/786 [==============================] - 0s 59us/step - loss: 0.9471 - acc: 0.6412\n",
      "Epoch 59/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.9651 - acc: 0.6285\n",
      "Epoch 60/150\n",
      "786/786 [==============================] - 0s 58us/step - loss: 1.0029 - acc: 0.6094\n",
      "Epoch 61/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.9479 - acc: 0.6361\n",
      "Epoch 62/150\n",
      "786/786 [==============================] - 0s 58us/step - loss: 0.9277 - acc: 0.6476\n",
      "Epoch 63/150\n",
      "786/786 [==============================] - 0s 59us/step - loss: 0.9075 - acc: 0.6387\n",
      "Epoch 64/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.8866 - acc: 0.6743\n",
      "Epoch 65/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.8828 - acc: 0.6527\n",
      "Epoch 66/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.8952 - acc: 0.6501\n",
      "Epoch 67/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.9464 - acc: 0.6323\n",
      "Epoch 68/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.9688 - acc: 0.6107\n",
      "Epoch 69/150\n",
      "786/786 [==============================] - 0s 73us/step - loss: 0.8620 - acc: 0.6781\n",
      "Epoch 70/150\n",
      "786/786 [==============================] - 0s 77us/step - loss: 0.8337 - acc: 0.6908\n",
      "Epoch 71/150\n",
      "786/786 [==============================] - 0s 71us/step - loss: 0.8683 - acc: 0.6794\n",
      "Epoch 72/150\n",
      "786/786 [==============================] - 0s 91us/step - loss: 0.8371 - acc: 0.6807\n",
      "Epoch 73/150\n",
      "786/786 [==============================] - 0s 83us/step - loss: 0.8578 - acc: 0.6679\n",
      "Epoch 74/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 0.8528 - acc: 0.6476\n",
      "Epoch 75/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.8899 - acc: 0.6667\n",
      "Epoch 76/150\n",
      "786/786 [==============================] - 0s 71us/step - loss: 0.8898 - acc: 0.6374\n",
      "Epoch 77/150\n",
      "786/786 [==============================] - 0s 69us/step - loss: 0.9405 - acc: 0.6374\n",
      "Epoch 78/150\n",
      "786/786 [==============================] - 0s 71us/step - loss: 0.8032 - acc: 0.6756\n",
      "Epoch 79/150\n",
      "786/786 [==============================] - 0s 80us/step - loss: 0.8355 - acc: 0.6934\n",
      "Epoch 80/150\n",
      "786/786 [==============================] - 0s 88us/step - loss: 0.8492 - acc: 0.6858\n",
      "Epoch 81/150\n",
      "786/786 [==============================] - 0s 74us/step - loss: 0.8798 - acc: 0.6654\n",
      "Epoch 82/150\n",
      "786/786 [==============================] - 0s 71us/step - loss: 0.8212 - acc: 0.6756\n",
      "Epoch 83/150\n",
      "786/786 [==============================] - 0s 71us/step - loss: 0.8374 - acc: 0.7023\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786/786 [==============================] - 0s 85us/step - loss: 0.7769 - acc: 0.7201\n",
      "Epoch 85/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.7863 - acc: 0.7099\n",
      "Epoch 86/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.8306 - acc: 0.6858\n",
      "Epoch 87/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.7901 - acc: 0.6781\n",
      "Epoch 88/150\n",
      "786/786 [==============================] - 0s 92us/step - loss: 0.7622 - acc: 0.6959\n",
      "Epoch 89/150\n",
      "786/786 [==============================] - 0s 69us/step - loss: 0.7655 - acc: 0.7137\n",
      "Epoch 90/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.7846 - acc: 0.6743\n",
      "Epoch 91/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 0.7361 - acc: 0.7163\n",
      "Epoch 92/150\n",
      "786/786 [==============================] - 0s 81us/step - loss: 0.7589 - acc: 0.6921\n",
      "Epoch 93/150\n",
      "786/786 [==============================] - 0s 88us/step - loss: 0.7431 - acc: 0.7061\n",
      "Epoch 94/150\n",
      "786/786 [==============================] - 0s 100us/step - loss: 0.7810 - acc: 0.7061\n",
      "Epoch 95/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.7813 - acc: 0.7061\n",
      "Epoch 96/150\n",
      "786/786 [==============================] - 0s 67us/step - loss: 0.7192 - acc: 0.7112\n",
      "Epoch 97/150\n",
      "786/786 [==============================] - 0s 74us/step - loss: 0.7318 - acc: 0.7099\n",
      "Epoch 98/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 0.6994 - acc: 0.7290\n",
      "Epoch 99/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.8493 - acc: 0.6628\n",
      "Epoch 100/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.7922 - acc: 0.6654\n",
      "Epoch 101/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 0.7741 - acc: 0.7087\n",
      "Epoch 102/150\n",
      "786/786 [==============================] - 0s 68us/step - loss: 0.7636 - acc: 0.6959\n",
      "Epoch 103/150\n",
      "786/786 [==============================] - 0s 84us/step - loss: 0.7206 - acc: 0.7099\n",
      "Epoch 104/150\n",
      "786/786 [==============================] - 0s 89us/step - loss: 0.8041 - acc: 0.6743\n",
      "Epoch 105/150\n",
      "786/786 [==============================] - 0s 81us/step - loss: 0.7492 - acc: 0.7125\n",
      "Epoch 106/150\n",
      "786/786 [==============================] - 0s 89us/step - loss: 0.6814 - acc: 0.7443\n",
      "Epoch 107/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.7359 - acc: 0.7099\n",
      "Epoch 108/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.7290 - acc: 0.7163\n",
      "Epoch 109/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.7339 - acc: 0.7252\n",
      "Epoch 110/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.6881 - acc: 0.7226\n",
      "Epoch 111/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.7311 - acc: 0.7214\n",
      "Epoch 112/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.7512 - acc: 0.7137\n",
      "Epoch 113/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.7289 - acc: 0.7214\n",
      "Epoch 114/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.6659 - acc: 0.7176\n",
      "Epoch 115/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.7212 - acc: 0.7048\n",
      "Epoch 116/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.6951 - acc: 0.7328\n",
      "Epoch 117/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.6937 - acc: 0.7290\n",
      "Epoch 118/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.7498 - acc: 0.7112\n",
      "Epoch 119/150\n",
      "786/786 [==============================] - 0s 59us/step - loss: 0.7182 - acc: 0.7379\n",
      "Epoch 120/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 0.6892 - acc: 0.7163\n",
      "Epoch 121/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.6569 - acc: 0.7430\n",
      "Epoch 122/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.6645 - acc: 0.7405\n",
      "Epoch 123/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.7468 - acc: 0.7099\n",
      "Epoch 124/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.7014 - acc: 0.7328\n",
      "Epoch 125/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.6899 - acc: 0.7341\n",
      "Epoch 126/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.6698 - acc: 0.7595\n",
      "Epoch 127/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.6407 - acc: 0.7595\n",
      "Epoch 128/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.7269 - acc: 0.7061\n",
      "Epoch 129/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.6663 - acc: 0.7506\n",
      "Epoch 130/150\n",
      "786/786 [==============================] - 0s 65us/step - loss: 0.7200 - acc: 0.7341\n",
      "Epoch 131/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.6710 - acc: 0.7506\n",
      "Epoch 132/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.6757 - acc: 0.7532\n",
      "Epoch 133/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.6089 - acc: 0.7748\n",
      "Epoch 134/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.6221 - acc: 0.7697\n",
      "Epoch 135/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.6391 - acc: 0.7506\n",
      "Epoch 136/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.6728 - acc: 0.7468\n",
      "Epoch 137/150\n",
      "786/786 [==============================] - 0s 58us/step - loss: 0.6373 - acc: 0.7494\n",
      "Epoch 138/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.6451 - acc: 0.7494\n",
      "Epoch 139/150\n",
      "786/786 [==============================] - 0s 60us/step - loss: 0.6675 - acc: 0.7252\n",
      "Epoch 140/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.6563 - acc: 0.7392\n",
      "Epoch 141/150\n",
      "786/786 [==============================] - 0s 66us/step - loss: 0.5845 - acc: 0.7824\n",
      "Epoch 142/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.6134 - acc: 0.7697\n",
      "Epoch 143/150\n",
      "786/786 [==============================] - 0s 61us/step - loss: 0.6116 - acc: 0.7634\n",
      "Epoch 144/150\n",
      "786/786 [==============================] - 0s 64us/step - loss: 0.6710 - acc: 0.7430\n",
      "Epoch 145/150\n",
      "786/786 [==============================] - 0s 63us/step - loss: 0.6433 - acc: 0.7468\n",
      "Epoch 146/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.6070 - acc: 0.7659\n",
      "Epoch 147/150\n",
      "786/786 [==============================] - 0s 62us/step - loss: 0.6089 - acc: 0.7723\n",
      "Epoch 148/150\n",
      "786/786 [==============================] - 0s 70us/step - loss: 0.6165 - acc: 0.7443\n",
      "Epoch 149/150\n",
      "786/786 [==============================] - 0s 69us/step - loss: 0.6174 - acc: 0.7608\n",
      "Epoch 150/150\n",
      "786/786 [==============================] - 0s 75us/step - loss: 0.6116 - acc: 0.7557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce9a1fe048>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(trainX, y_train, epochs=150, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 0s 809us/step\n",
      "[1.491453538691332, 0.5329949240091488]\n"
     ]
    }
   ],
   "source": [
    "accuracy = model1.evaluate(testX, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* features extraction was not usefull as we have seen. we noticed from the data behaviour that the value of the peaks matters more than the number of peaks.(to be considered in further work)\n",
    "* ECG data was giving acuraccy of 58% at some moments which is better than the other trials. but still we will try to increase the accuracy in 2D CNN   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "* TuanNguyen GiaaImed Ben Dhaoub Mai Alic Amir M.Rahmanide Tomi Wester lunda Pasi Liljeberga Hannu Tenhunena. “Energy efficient fog-assisted IoT system for monitoring diabetic patients with cardiovascular disease”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
