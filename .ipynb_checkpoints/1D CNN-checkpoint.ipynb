{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Matplotlib forms basis for visualization in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We will use the Seaborn library\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Graphics in SVG format are more sharp and legible\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4          5        6   \\\n",
      "0       -9.4674  0.137210  1.054500  0.238620 -0.033490  -0.188480  -9.8425   \n",
      "1       -9.5991  0.154810  1.182400  0.226060 -0.079540  -0.362630  -9.7832   \n",
      "2       -9.8612  0.133170  1.064600  0.205130 -0.096285  -0.248230  -9.8010   \n",
      "3       -9.6398  0.174060  1.236700  0.184200 -0.062794  -0.250270  -9.6730   \n",
      "4      -10.0360  0.260620  1.276100  0.125590 -0.025118  -0.206680  -9.9102   \n",
      "5       -9.6700  0.167460  0.857180  0.062794 -0.033490  -0.390890  -9.9108   \n",
      "6       -9.2655  0.650970  1.395400  0.025118 -0.041863  -0.105080  -9.8407   \n",
      "7       -9.6621  0.277490  0.972340  0.012559 -0.037677  -0.495030  -9.8329   \n",
      "8       -9.4799  0.257660  1.117900  0.020931 -0.025118  -0.106760  -9.7624   \n",
      "9       -9.6593  0.134870  1.096300  0.025118 -0.020931  -0.065413  -9.8618   \n",
      "10      -9.6210  0.243950  1.337700  0.025118 -0.025118  -0.126850  -9.8621   \n",
      "11      -9.6293  0.154090  1.224400  0.012559 -0.041863  -0.206950  -9.6405   \n",
      "12      -9.5744 -0.067700  1.236000  0.008373 -0.041863  -0.350770  -9.7815   \n",
      "13      -9.1642  0.128990  1.169500 -0.004186 -0.054422  -0.247470  -9.8802   \n",
      "14      -9.3535 -0.004205  1.168000 -0.016745 -0.071167  -0.236710  -9.8296   \n",
      "15      -9.7328  0.276890  0.961760 -0.004186 -0.041863  -0.217500  -9.9109   \n",
      "16      -9.4566  0.105040  1.251600 -0.016745 -0.075353  -0.277700  -9.7898   \n",
      "17      -9.7576  0.001639  1.189600 -0.016745 -0.054422  -0.491340  -9.9692   \n",
      "18      -9.7179  0.034731  0.950490  0.000000 -0.041863  -0.339570  -9.9305   \n",
      "19      -9.6393  0.145370  1.076200  0.033490 -0.033490  -0.361800  -9.7723   \n",
      "20      -9.7245  0.365380  1.211800  0.050235 -0.016745  -0.370720  -9.6110   \n",
      "21      -9.2213 -0.045356  1.374900 -0.050235 -0.079540  -0.145490  -9.8403   \n",
      "22      -9.6786  0.091257  1.397700 -0.046049 -0.108840  -0.265440 -10.0280   \n",
      "23      -9.6177  0.074837  1.080400 -0.066981 -0.129770  -0.247900 -10.0110   \n",
      "24      -9.4476  0.157590  1.056300 -0.071167 -0.108840  -0.307170  -9.8987   \n",
      "25      -9.8208  0.131680  1.254000 -0.154890 -0.138150  -0.627350  -9.6725   \n",
      "26      -9.7406  0.163280  1.204200  0.552590  0.113030  -0.278450  -9.7106   \n",
      "27      -9.7744 -0.170620  1.226700  3.127200  1.754100  -0.150970 -10.0860   \n",
      "28      -9.7482  0.034533  0.939950 -0.330720 -0.125590  -0.272920  -9.9850   \n",
      "29      -9.3760  0.115200  1.326200 -0.125590 -0.092098  -0.276830  -9.7489   \n",
      "...         ...       ...       ...       ...       ...        ...      ...   \n",
      "122082  -5.4386 -0.289920 -0.689250 -1.276800  0.117220   0.157380  -4.8143   \n",
      "122083  -6.0157  0.278710  0.456660 -1.281000  0.163270   1.343100  -4.5025   \n",
      "122084  -6.5886  0.135800  0.033275 -1.222400  0.259550   0.982270  -5.5763   \n",
      "122085  -8.2397  0.369920  0.222770 -1.151200  0.376770   1.228000  -6.4956   \n",
      "122086  -8.1721  0.525760 -0.142160 -1.138700  0.422820   0.953100  -8.0845   \n",
      "122087  -7.9731  0.676390  0.145090 -1.142900  0.452120   0.666870  -9.1848   \n",
      "122088  -7.7674  0.996660 -0.362930 -1.109400  0.506540   0.845210 -11.8000   \n",
      "122089  -8.6483  1.100800  0.467490 -1.121900  0.523290   0.497940 -14.7400   \n",
      "122090  -9.1212  0.504180  1.066400 -1.205700  0.514910   2.216400 -17.8250   \n",
      "122091  -9.3978  0.194360  0.459600 -1.247500  0.531660   1.613100 -19.3070   \n",
      "122092 -10.2760  0.680190  0.492800 -1.268400  0.556780  -7.450600 -19.4110   \n",
      "122093 -10.7320  1.749800  0.146960 -1.272600  0.615380  -5.695400 -19.4630   \n",
      "122094 -11.3070  1.192600 -0.261090 -1.235000  0.632130  -2.294900 -12.8520   \n",
      "122095 -12.8110  0.646040 -0.647120 -1.193100  0.661430  -8.659200  -5.7505   \n",
      "122096 -13.9410 -0.431310 -1.112500 -1.343800  0.594450  19.466000 -12.1960   \n",
      "122097 -14.2530 -2.004900 -2.654100 -1.406600  0.560960  -4.443200 -13.4060   \n",
      "122098 -13.1350 -0.802020 -4.651100 -1.440100  0.556780 -19.515000 -19.2370   \n",
      "122099  -9.7995  1.086900 -3.333800 -1.578200  0.527470  -2.672600 -19.0800   \n",
      "122100  -8.3786  0.241870 -3.273000  1.858700  2.495000  -4.984000 -17.8020   \n",
      "122101  -8.0139  0.697340 -4.010900 -1.858700  0.355830   4.089800  -8.3598   \n",
      "122102  -9.3850  0.055839 -2.847600 -1.595000  0.527470  -2.843000  -9.4345   \n",
      "122103 -11.5580 -0.364930 -3.348600 -1.607500  0.527470   4.569800 -10.7800   \n",
      "122104 -10.5230 -1.103900 -3.162500 -1.582400  0.527470   3.222600 -10.7600   \n",
      "122105  -9.9442 -0.264810 -2.655400 -1.548900  0.560960   1.747300 -10.7260   \n",
      "122106  -9.1037 -0.372050 -2.201900 -1.431700  0.636320   1.775800 -10.2880   \n",
      "122107  -7.9261  0.344050 -1.926900 -1.385700  0.715860   1.539200  -9.7096   \n",
      "122108  -7.3952  0.064886 -1.762600 -1.348000  0.720040   2.072100  -9.7372   \n",
      "122109  -7.7164 -0.543520 -1.848900 -1.385700  0.673990   2.058600  -9.7005   \n",
      "122110  -9.4886 -1.326700 -2.066800 -1.322900  0.644690   2.249000  -9.7241   \n",
      "122111  -9.9039 -1.773400 -2.264900 -1.281000  0.703300   1.748200  -9.3951   \n",
      "\n",
      "               7        8        9  ...        14       15        16  \\\n",
      "0        0.901830  0.55659 -0.83490 ...  -9.08970   2.0431  1.310500   \n",
      "1        0.892630  0.55659 -0.83490 ...  -9.60100   1.8242  1.424500   \n",
      "2        1.090800  0.57885 -0.84615 ...  -8.74460   2.4401  1.007400   \n",
      "3        1.013800  0.57885 -0.84615 ...  -9.89470   1.9019  1.188800   \n",
      "4        1.054500  0.57885 -0.84615 ...  -9.65220   2.0140  1.402800   \n",
      "5        1.000600  0.55102 -0.83865 ...  -9.59630   2.3111  1.105100   \n",
      "6        1.079800  0.55102 -0.83865 ...  -9.80590   2.4704  1.040200   \n",
      "7        0.867990  0.55102 -0.83865 ...  -9.69360   2.4597  0.966730   \n",
      "8        0.989980  0.55102 -0.83865 ...  -9.37930   2.2788  0.884780   \n",
      "9        0.943750  0.56772 -0.83114 ... -10.03800   2.0725  1.249500   \n",
      "10       0.922290  0.56772 -0.83114 ...  -9.64160   2.0404  1.039500   \n",
      "11       1.297200  0.56772 -0.83114 ...  -9.57920   1.9997  0.974990   \n",
      "12       1.059500  0.56401 -0.84615 ...  -9.59630   1.8559  0.592800   \n",
      "13       1.086800  0.56401 -0.84615 ...  -9.31670   1.8893  0.927800   \n",
      "14       1.193700  0.56401 -0.84615 ...  -8.94270   1.7584  0.837920   \n",
      "15       0.991860  0.56401 -0.84615 ...  -9.36110   1.6482  0.821990   \n",
      "16       1.216100  0.56586 -0.82927 ...  -9.48160   1.5786  0.860180   \n",
      "17       1.100800  0.56586 -0.82927 ...  -9.58230   1.4194  0.935550   \n",
      "18       1.010500  0.56586 -0.82927 ...  -9.22810   1.6040  0.403120   \n",
      "19       0.986930  0.56957 -0.82552 ...  -9.29930   1.4881  0.813990   \n",
      "20       1.276200  0.56957 -0.82552 ...  -9.20330   1.3998  0.976240   \n",
      "21       1.121100  0.56957 -0.82552 ... -10.09800   1.5640  1.401100   \n",
      "22       1.183500  0.56586 -0.84053 ...  -9.37340   1.7088  0.877110   \n",
      "23       0.913650  0.56586 -0.84053 ...  -9.60830   1.7802  1.016100   \n",
      "24       1.220700  0.56586 -0.84053 ...  -9.31480   1.6395  0.946560   \n",
      "25       1.062200  0.56586 -0.84053 ...  -9.34000   1.4684  0.844260   \n",
      "26       1.220000  0.56586 -0.83490 ...  -9.36600   1.4206  1.060600   \n",
      "27       1.348400  0.56586 -0.83490 ...  -9.41040   1.3423  1.233800   \n",
      "28       1.498000  0.56586 -0.83490 ...  -9.59590   1.4638  1.375300   \n",
      "29       1.343100  0.55102 -0.84240 ...  -9.48610   1.7030  1.303900   \n",
      "...           ...      ...      ... ...       ...      ...       ...   \n",
      "122082   1.740000  0.50464 -0.16698 ...  -3.03010 -18.6310 -0.119060   \n",
      "122083   1.236200  0.50464 -0.16698 ...  -3.17940 -18.5880  0.174750   \n",
      "122084  -0.206490  0.53061 -0.24015 ...  -2.48370 -17.5340 -0.366270   \n",
      "122085  -1.041700  0.53061 -0.24015 ...  -3.16010 -16.5380 -0.816490   \n",
      "122086  -0.530110  0.53061 -0.24015 ...  -3.13550 -15.5230 -0.307400   \n",
      "122087  -0.660910  0.54917 -0.40525 ...  -3.20290 -14.4510 -0.087127   \n",
      "122088  -1.769000  0.54917 -0.40525 ...  -3.59220 -13.3380 -0.812740   \n",
      "122089  -2.700800  0.54917 -0.40525 ...  -3.94230 -11.6440 -1.373900   \n",
      "122090  -3.327400  0.54917 -0.40525 ...  -4.72500 -10.5960 -1.634000   \n",
      "122091  -6.980500  0.51948 -0.62101 ...  -4.59850  -9.5354 -1.539300   \n",
      "122092 -10.389000  0.51948 -0.62101 ...  -5.22180  -8.6772 -0.715110   \n",
      "122093 -16.551000  0.51948 -0.62101 ...  -5.32960  -7.9736 -0.357010   \n",
      "122094 -18.874000  0.50093 -0.69981 ...  -3.77850  -7.6550  0.502670   \n",
      "122095 -18.614000  0.50093 -0.69981 ...  -2.69570  -6.9250  0.497500   \n",
      "122096 -18.644000  0.50093 -0.69981 ...  -3.22650  -5.8450  0.499420   \n",
      "122097 -12.829000  0.50093 -0.69981 ...  -4.16360  -5.4241  0.593380   \n",
      "122098  10.905000  0.37662 -0.71295 ...  -3.77590  -4.6525  1.746500   \n",
      "122099  18.759000  0.37662 -0.71295 ...  -3.78730  -4.3963  3.373900   \n",
      "122100  -2.939900  0.37662 -0.71295 ...  -3.17620  -4.6823  3.765900   \n",
      "122101   1.643700  0.44527 -0.64728 ...  -4.10770  -4.9542  4.575200   \n",
      "122102  -0.875150  0.44527 -0.64728 ...  -4.37710  -5.0342  4.581600   \n",
      "122103  -1.764100  0.44527 -0.64728 ...  -4.64310  -4.6800  5.995200   \n",
      "122104  -0.745350  0.39518 -0.58724 ...  -3.30460  -4.6264  7.362400   \n",
      "122105  -0.279220  0.39518 -0.58724 ...  -1.43630  -4.5627  7.730200   \n",
      "122106  -0.059128  0.39518 -0.58724 ...  -1.86910  -5.3259  7.410800   \n",
      "122107   0.332000  0.39518 -0.58724 ...  -3.46300  -6.7532  7.683200   \n",
      "122108   0.533860  0.37477 -0.57036 ...   0.41986  -6.6179  7.210800   \n",
      "122109   0.243890  0.37477 -0.57036 ...   0.60193  -6.7714  7.859300   \n",
      "122110  -0.130380  0.37477 -0.57036 ...   1.29320  -6.4462  8.375600   \n",
      "122111  -0.900190  0.34323 -0.57786 ...   1.12920  -6.3951  9.492700   \n",
      "\n",
      "              17       18        19          20        21          22  23  \n",
      "0      -0.754900 -0.22998  0.782330   -1.605100   1.06850   -3.652900   0  \n",
      "1      -0.754900 -0.22998  0.782330   -2.101500   5.29040    0.627090   0  \n",
      "2      -0.754900 -0.22998  0.782330    0.565620   2.70210   -1.459800   0  \n",
      "3      -0.754900 -0.22998  0.782330    0.244770   6.52330   -0.782360   0  \n",
      "4      -0.752940 -0.22998  0.762930    0.034368   3.43680   -1.116700   0  \n",
      "5      -0.752940 -0.22998  0.762930    0.215810   3.62730   -0.031957   0  \n",
      "6      -0.752940 -0.22998  0.762930   -1.368900   6.73500    0.627290   0  \n",
      "7      -0.770590 -0.22793  0.745690   -4.433000   5.53090    4.188200   0  \n",
      "8      -0.770590 -0.22793  0.745690   -5.660700   8.43920    3.412500   0  \n",
      "9      -0.770590 -0.22793  0.745690   -6.928200   7.36280    3.397900   0  \n",
      "10     -0.770590 -0.22793  0.745690   -6.076200   2.78880    0.572610   0  \n",
      "11     -0.766670 -0.24435  0.734910   -6.049100   5.50700    0.184960   0  \n",
      "12     -0.766670 -0.24435  0.734910   -7.115400   6.59180   -1.291300   0  \n",
      "13     -0.766670 -0.24435  0.734910   -7.470900   6.94740   -2.385000   0  \n",
      "14     -0.788240 -0.24025  0.711210   -8.002400   7.65680   -4.568700   0  \n",
      "15     -0.788240 -0.24025  0.711210   -8.651600  14.54800   -5.733700   0  \n",
      "16     -0.788240 -0.24025  0.711210   -9.078000   7.81640   -7.841000   0  \n",
      "17     -0.788240 -0.24025  0.711210   -8.044600   3.43650   -9.220500   0  \n",
      "18     -0.786270 -0.27926  0.711210   -7.328500   3.24080   -9.926300   0  \n",
      "19     -0.786270 -0.27926  0.711210   -6.461600   0.15815   -8.795100   0  \n",
      "20     -0.786270 -0.27926  0.711210   -5.560600   0.49783  -10.224000   0  \n",
      "21     -0.780390 -0.29363  0.696120   -3.218900   1.26570   -3.687100   0  \n",
      "22     -0.780390 -0.29363  0.696120   -1.438300  -0.19986   -3.275900   0  \n",
      "23     -0.780390 -0.29363  0.696120    0.505740  -3.28590   -1.401100   0  \n",
      "24     -0.780390 -0.29363  0.696120    2.103500  -5.09460    0.815040   0  \n",
      "25     -0.784310 -0.30390  0.704740    3.713800  -5.64030    2.296700   0  \n",
      "26     -0.784310 -0.30390  0.704740    4.067500  -6.18100    3.031200   0  \n",
      "27     -0.784310 -0.30390  0.704740    5.326000  -6.00100    4.137600   0  \n",
      "28     -0.776470 -0.28542  0.700430    5.873700  -5.08820    5.222500   0  \n",
      "29     -0.776470 -0.28542  0.700430    6.584600  -5.81740    5.605000   0  \n",
      "...          ...      ...       ...         ...       ...         ...  ..  \n",
      "122082  0.892160 -0.70637  0.017241   99.231000 -58.98600  226.040000   0  \n",
      "122083  0.892160 -0.70637  0.017241  103.690000 -61.63300  238.070000   0  \n",
      "122084  0.678430 -0.92402  0.075431  106.780000 -57.73300  250.010000   0  \n",
      "122085  0.678430 -0.92402  0.075431  107.560000 -51.63300  261.870000   0  \n",
      "122086  0.678430 -0.92402  0.075431  105.240000 -50.47100  266.870000   0  \n",
      "122087  0.678430 -0.92402  0.075431  103.100000 -48.99800  266.810000   0  \n",
      "122088  0.400000 -1.06160  0.239220   99.586000 -41.37800  263.060000   0  \n",
      "122089  0.400000 -1.06160  0.239220   90.843000 -35.94600  253.080000   0  \n",
      "122090  0.400000 -1.06160  0.239220   81.130000 -37.81700  237.740000   0  \n",
      "122091  0.147060 -1.08830  0.420260   71.230000 -40.41600  222.050000   0  \n",
      "122092  0.147060 -1.08830  0.420260   62.102000 -37.57400  206.670000   0  \n",
      "122093  0.147060 -1.08830  0.420260   55.792000 -40.18700  193.210000   0  \n",
      "122094  0.147060 -1.08830  0.420260   51.818000 -42.64400  179.440000   0  \n",
      "122095 -0.058824 -1.05130  0.594830   49.918000 -35.11900  167.780000   0  \n",
      "122096 -0.058824 -1.05130  0.594830   45.195000 -22.68100  154.560000   0  \n",
      "122097 -0.058824 -1.05130  0.594830   35.854000 -23.26100  141.740000   0  \n",
      "122098 -0.166670 -0.99589  0.719830   24.318000 -27.83200  126.760000   0  \n",
      "122099 -0.166670 -0.99589  0.719830   14.006000 -35.68600  111.470000   0  \n",
      "122100 -0.166670 -0.99589  0.719830    5.360200 -38.48600   95.077000   0  \n",
      "122101 -0.166670 -0.99589  0.719830   -4.349500 -39.98000   81.179000   0  \n",
      "122102 -0.201960 -0.97536  0.799570  -16.250000 -45.10700   64.747000   0  \n",
      "122103 -0.201960 -0.97536  0.799570  -26.258000 -58.49500   58.905000   0  \n",
      "122104 -0.201960 -0.97536  0.799570  -26.767000 -73.43700   70.596000   0  \n",
      "122105 -0.133330 -1.01440  0.788790  -27.565000 -81.45800   84.379000   0  \n",
      "122106 -0.133330 -1.01440  0.788790  -31.801000 -74.15800  102.270000   0  \n",
      "122107 -0.133330 -1.01440  0.788790  -31.169000 -82.74900  115.010000   0  \n",
      "122108 -0.133330 -1.01440  0.788790  -27.081000 -86.96300  115.130000   0  \n",
      "122109 -0.105880 -1.07390  0.767240  -25.230000 -63.37900  116.010000   0  \n",
      "122110 -0.105880 -1.07390  0.767240  -20.491000 -38.34600  119.470000   0  \n",
      "122111 -0.105880 -1.07390  0.767240  -13.590000 -12.45700  120.070000   0  \n",
      "\n",
      "[122112 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mHealth_subject3.log\", header=None, delim_whitespace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the samples size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples of 0 activity is (86732, 24)\n",
      "number of samples of 1 activity is (3072, 24)\n",
      "number of samples of 2 activity is (3072, 24)\n",
      "number of samples of 3 activity is (3072, 24)\n",
      "number of samples of 4 activity is (3072, 24)\n",
      "number of samples of 5 activity is (3072, 24)\n",
      "number of samples of 6 activity is (3226, 24)\n",
      "number of samples of 7 activity is (3379, 24)\n",
      "number of samples of 8 activity is (3175, 24)\n",
      "number of samples of 9 activity is (3072, 24)\n",
      "number of samples of 10 activity is (3072, 24)\n",
      "number of samples of 11 activity is (3072, 24)\n",
      "number of samples of 12 activity is (1024, 24)\n"
     ]
    }
   ],
   "source": [
    "for i in range(13): \n",
    "    df1=df[df[23] == i]\n",
    "    print('number of samples of', i,\"activity is\", df1.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we found it better to exclude the labels with different size to end up with 8 classes rether than 12   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df[23] != 6]\n",
    "df=df[df[23] != 7]\n",
    "df=df[df[23] != 8]\n",
    "df=df[df[23] != 12]\n",
    "df=df[df[23] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract ECG column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractECG(dataFrame,lead=1):\n",
    "    if lead == 1:\n",
    "        lead = 4\n",
    "    else :\n",
    "        lead = 5\n",
    "        \n",
    "    data = dataFrame[lead]\n",
    "#     data = data.reset_index()\n",
    "#     data.columns=[\"index\",\"readings\"]\n",
    "    return data\n",
    "data=extractECG(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine labels based on frequency\n",
    "for example for one seconds classification every 50 readings belongs to one label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "readings=25\n",
    "def fmax(label_count):\n",
    "    M=0\n",
    "    for i in range (int (len (label_count)) ):\n",
    "        if label_count[i]>label_count[M]:\n",
    "            M=i\n",
    "    return M\n",
    "labels=[]\n",
    "label_index=0\n",
    "label = df[23].values\n",
    "label_count=[0,0,0,0,0,0,0,0]\n",
    "for i in range(int(len(label))):\n",
    "    if label[i]==1:\n",
    "        label_count[0]=label_count[0]+1\n",
    "    elif label[i]==2:\n",
    "        label_count[1]=label_count[1]+1\n",
    "    elif label[i]==3:\n",
    "        label_count[2]=label_count[2]+1\n",
    "    elif label[i]==4:\n",
    "        label_count[3]=label_count[3]+1\n",
    "    elif label[i]==5:\n",
    "        label_count[4]=label_count[4]+1\n",
    "    elif label[i]==9:\n",
    "        label_count[5]=label_count[5]+1\n",
    "    elif label[i]==10:\n",
    "        label_count[6]=label_count[6]+1\n",
    "    elif label[i]==11:\n",
    "        label_count[7]=label_count[7]+1\n",
    "    \n",
    "    if(i%readings==readings-1):\n",
    "        M=fmax(label_count)\n",
    "        labels.append(M)\n",
    "        label_count=[0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        label_index=label_index+1\n",
    "labels=np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "based on some research we found that there are many features that can be measured from the ecg signal. we choose 2 features that are possible to be implemented.\n",
    "* number of peaks in the signal\n",
    "* heartrate[1] \n",
    "\n",
    "We will see if these 2 features are useful for prediction or not.\n",
    "\n",
    "The following 2 blocks contain functions for features extraction\n",
    "\n",
    "The heartrate is not measured to be beat/min. Instead it is dependent on number of readings we specify. As we know the data has a frequency of 50 readings/sec so when we use 25 readings the heartrate unit will be beat/halfsecond and so on    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmax(ecg):\n",
    "    \n",
    "    M=0\n",
    "    \n",
    "    index=0 \n",
    "    for i in range(int(len(ecg))):\n",
    "        if M < ecg[i]:\n",
    "            M=ecg[i]\n",
    "            index=i\n",
    "        \n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def findmaxindex(ecg,peaks):\n",
    "    \n",
    "    M=0\n",
    "    \n",
    "    index=0\n",
    "    for i in range(int(len(peaks))):\n",
    "        if M < ecg[peaks[i]]:\n",
    "            M=ecg[peaks[i]]\n",
    "            index=i\n",
    "        \n",
    "    \n",
    "    return index\n",
    "\n",
    "def findpeak(ecg):\n",
    "    \n",
    "    start=0\n",
    "    end=10\n",
    "    \n",
    "    peaks=[]\n",
    "  \n",
    "    for i in range(int(len(ecg)/10)):\n",
    "        ecg1=ecg[start:end]\n",
    "        ecg1=ecg1.reset_index(drop=True)\n",
    "        peaks.append(findmax(ecg1))\n",
    "        start=start+10\n",
    "        end=end+10\n",
    "        \n",
    "    return peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks1=findpeaksper2second(data[100:200])\n",
    "# print(peaks1)\n",
    "\n",
    "\n",
    "\n",
    "heartrate=[]\n",
    "numbofpeaks=[]\n",
    "\n",
    "def arrange(peaks):\n",
    "    for i in range(int(len(peaks))):\n",
    "        if peaks[i]!=-1 and peaks[i]!=0:\n",
    "            peaks[i]=i*10+peaks[i]\n",
    "            \n",
    "    return peaks\n",
    "    \n",
    "for i in range (int(len(data)/readings)):\n",
    "    data1=data[i*readings:(i+1)*readings]\n",
    "    data1=data1.reset_index(drop=True)\n",
    "    \n",
    "    peaks1=findpeak(data1)\n",
    "#     print (peaks1)\n",
    "    maxindex=findmaxindex(data1,peaks1)\n",
    "        \n",
    "#     print(peaks1)    \n",
    "    for j in range (int(len(peaks1))):\n",
    "            \n",
    "        if data1[peaks1[maxindex]]-data1[peaks1[j]]>0.7 :\n",
    "            peaks1[j]=-1\n",
    "#     print(peaks1)\n",
    "    \n",
    "    n=0\n",
    "    for j in range (int(len(peaks1))):\n",
    "        if peaks1[j]!=-1 or peaks1[j]!=0:\n",
    "            n=n+1\n",
    "    numbofpeaks.append(n)\n",
    "    peaks1=arrange(peaks1)\n",
    "#     print(peaks1)\n",
    "#     break\n",
    "    firstpeak=0\n",
    "    secondpeak=0\n",
    "    for j in range (int(len(peaks1))):\n",
    "        if peaks1[j]!=-1 or peaks1[j]!=0:\n",
    "            if firstpeak==0:\n",
    "                firstpeak=peaks1[j]\n",
    "            elif secondpeak==0:\n",
    "                secondpeak=peaks1[j]\n",
    "            else :\n",
    "                break\n",
    "    hr=abs(((firstpeak-secondpeak))/readings)\n",
    "    heartrate.append(hr)\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the ECG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.values\n",
    "labels1=df[23].values\n",
    "for j in range(data.size%readings):\n",
    "    data = np.delete(data, (j), axis=0)\n",
    "data= data.reshape(int(data.size/readings),1,readings)\n",
    "numbofpeaks=np.array(numbofpeaks)\n",
    "numbofpeaks= numbofpeaks.reshape(int(numbofpeaks.size),1,1)\n",
    "heartrate=np.array(heartrate)\n",
    "heartrate= heartrate.reshape(int(heartrate.size),1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 types of data that can be used for models   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spliting the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since it is a time series data it would be better to split the data by taking the first big part for training and the last small part for validation and testing. However, this can't be done because the last part of the data doesn't have the labels that the first part has. so different classes might appear in the testing set. Therefore, we used split function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(heartrate, to_categorical(labels), test_size = 0.20, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heartrate 1D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1, 64)             128       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 41,672\n",
      "Trainable params: 41,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(786, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "model.summary()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "786/786 [==============================] - 2s 2ms/step - loss: 2.0802 - acc: 0.1247\n",
      "Epoch 2/100\n",
      "786/786 [==============================] - 0s 198us/step - loss: 2.0778 - acc: 0.1489\n",
      "Epoch 3/100\n",
      "786/786 [==============================] - 0s 187us/step - loss: 2.0758 - acc: 0.1336\n",
      "Epoch 4/100\n",
      "786/786 [==============================] - 0s 321us/step - loss: 2.0680 - acc: 0.1603\n",
      "Epoch 5/100\n",
      "786/786 [==============================] - 0s 235us/step - loss: 2.0653 - acc: 0.1501\n",
      "Epoch 6/100\n",
      "786/786 [==============================] - 0s 224us/step - loss: 2.0583 - acc: 0.1476 0s - loss: 2.0500 - acc: 0.1\n",
      "Epoch 7/100\n",
      "786/786 [==============================] - 0s 198us/step - loss: 2.0549 - acc: 0.1425\n",
      "Epoch 8/100\n",
      "786/786 [==============================] - 0s 183us/step - loss: 2.0558 - acc: 0.1489\n",
      "Epoch 9/100\n",
      "786/786 [==============================] - 0s 214us/step - loss: 2.0441 - acc: 0.1679\n",
      "Epoch 10/100\n",
      "786/786 [==============================] - 0s 288us/step - loss: 2.0442 - acc: 0.1565\n",
      "Epoch 11/100\n",
      "786/786 [==============================] - 0s 193us/step - loss: 2.0440 - acc: 0.1590\n",
      "Epoch 12/100\n",
      "786/786 [==============================] - 0s 239us/step - loss: 2.0412 - acc: 0.1616\n",
      "Epoch 13/100\n",
      "786/786 [==============================] - 0s 230us/step - loss: 2.0389 - acc: 0.1539\n",
      "Epoch 14/100\n",
      "786/786 [==============================] - 0s 196us/step - loss: 2.0407 - acc: 0.1476\n",
      "Epoch 15/100\n",
      "786/786 [==============================] - 0s 202us/step - loss: 2.0327 - acc: 0.1450\n",
      "Epoch 16/100\n",
      "786/786 [==============================] - 0s 284us/step - loss: 2.0247 - acc: 0.1743\n",
      "Epoch 17/100\n",
      "786/786 [==============================] - 0s 181us/step - loss: 2.0254 - acc: 0.1705\n",
      "Epoch 18/100\n",
      "786/786 [==============================] - 0s 196us/step - loss: 2.0210 - acc: 0.1718\n",
      "Epoch 19/100\n",
      "786/786 [==============================] - 0s 222us/step - loss: 2.0337 - acc: 0.1628\n",
      "Epoch 20/100\n",
      "786/786 [==============================] - 0s 198us/step - loss: 2.0272 - acc: 0.1654\n",
      "Epoch 21/100\n",
      "786/786 [==============================] - 0s 198us/step - loss: 2.0250 - acc: 0.1679\n",
      "Epoch 22/100\n",
      "786/786 [==============================] - 0s 272us/step - loss: 2.0176 - acc: 0.1858\n",
      "Epoch 23/100\n",
      "786/786 [==============================] - 0s 172us/step - loss: 2.0249 - acc: 0.1692\n",
      "Epoch 24/100\n",
      "786/786 [==============================] - 0s 197us/step - loss: 2.0169 - acc: 0.1756\n",
      "Epoch 25/100\n",
      "786/786 [==============================] - 0s 230us/step - loss: 2.0149 - acc: 0.1692\n",
      "Epoch 26/100\n",
      "786/786 [==============================] - 0s 185us/step - loss: 2.0168 - acc: 0.1692\n",
      "Epoch 27/100\n",
      "786/786 [==============================] - 0s 210us/step - loss: 2.0070 - acc: 0.1756\n",
      "Epoch 28/100\n",
      "786/786 [==============================] - 0s 211us/step - loss: 2.0216 - acc: 0.1832\n",
      "Epoch 29/100\n",
      "786/786 [==============================] - 0s 281us/step - loss: 2.0123 - acc: 0.1781\n",
      "Epoch 30/100\n",
      "786/786 [==============================] - 0s 200us/step - loss: 2.0093 - acc: 0.1870\n",
      "Epoch 31/100\n",
      "786/786 [==============================] - 0s 248us/step - loss: 2.0115 - acc: 0.1819\n",
      "Epoch 32/100\n",
      "786/786 [==============================] - 0s 195us/step - loss: 2.0069 - acc: 0.1883 0s - loss: 2.0059 - acc: 0.19\n",
      "Epoch 33/100\n",
      "786/786 [==============================] - 0s 233us/step - loss: 2.0046 - acc: 0.2036\n",
      "Epoch 34/100\n",
      "786/786 [==============================] - 0s 237us/step - loss: 2.0100 - acc: 0.1870\n",
      "Epoch 35/100\n",
      "786/786 [==============================] - 0s 258us/step - loss: 2.0038 - acc: 0.1959\n",
      "Epoch 36/100\n",
      "786/786 [==============================] - 0s 191us/step - loss: 2.0075 - acc: 0.1756\n",
      "Epoch 37/100\n",
      "786/786 [==============================] - 0s 221us/step - loss: 2.0044 - acc: 0.1641\n",
      "Epoch 38/100\n",
      "786/786 [==============================] - 0s 197us/step - loss: 2.0029 - acc: 0.1870\n",
      "Epoch 39/100\n",
      "786/786 [==============================] - 0s 196us/step - loss: 2.0056 - acc: 0.1832\n",
      "Epoch 40/100\n",
      "786/786 [==============================] - 0s 211us/step - loss: 2.0027 - acc: 0.1832\n",
      "Epoch 41/100\n",
      "786/786 [==============================] - 0s 266us/step - loss: 2.0092 - acc: 0.1845\n",
      "Epoch 42/100\n",
      "786/786 [==============================] - 0s 214us/step - loss: 2.0046 - acc: 0.1807\n",
      "Epoch 43/100\n",
      "786/786 [==============================] - 0s 209us/step - loss: 2.0083 - acc: 0.1908\n",
      "Epoch 44/100\n",
      "786/786 [==============================] - 0s 203us/step - loss: 2.0026 - acc: 0.1870\n",
      "Epoch 45/100\n",
      "786/786 [==============================] - 0s 184us/step - loss: 2.0012 - acc: 0.1908\n",
      "Epoch 46/100\n",
      "786/786 [==============================] - 0s 226us/step - loss: 2.0036 - acc: 0.1819\n",
      "Epoch 47/100\n",
      "786/786 [==============================] - 0s 272us/step - loss: 2.0091 - acc: 0.1781\n",
      "Epoch 48/100\n",
      "786/786 [==============================] - 0s 202us/step - loss: 2.0043 - acc: 0.1768\n",
      "Epoch 49/100\n",
      "786/786 [==============================] - 0s 209us/step - loss: 1.9996 - acc: 0.1870\n",
      "Epoch 50/100\n",
      "786/786 [==============================] - 0s 247us/step - loss: 1.9996 - acc: 0.1794\n",
      "Epoch 51/100\n",
      "786/786 [==============================] - 0s 249us/step - loss: 1.9995 - acc: 0.1883\n",
      "Epoch 52/100\n",
      "786/786 [==============================] - 0s 223us/step - loss: 1.9999 - acc: 0.1883\n",
      "Epoch 53/100\n",
      "786/786 [==============================] - 0s 306us/step - loss: 2.0014 - acc: 0.1832\n",
      "Epoch 54/100\n",
      "786/786 [==============================] - 0s 234us/step - loss: 1.9990 - acc: 0.1908\n",
      "Epoch 55/100\n",
      "786/786 [==============================] - 0s 292us/step - loss: 2.0001 - acc: 0.1997\n",
      "Epoch 56/100\n",
      "786/786 [==============================] - 0s 272us/step - loss: 1.9995 - acc: 0.2023\n",
      "Epoch 57/100\n",
      "786/786 [==============================] - 0s 261us/step - loss: 2.0006 - acc: 0.1883\n",
      "Epoch 58/100\n",
      "786/786 [==============================] - 0s 296us/step - loss: 1.9982 - acc: 0.1934\n",
      "Epoch 59/100\n",
      "786/786 [==============================] - 0s 230us/step - loss: 1.9954 - acc: 0.1883 0s - loss: 1.9836 - acc: 0.19\n",
      "Epoch 60/100\n",
      "786/786 [==============================] - 0s 258us/step - loss: 2.0003 - acc: 0.1858\n",
      "Epoch 61/100\n",
      "786/786 [==============================] - 0s 249us/step - loss: 2.0008 - acc: 0.1845\n",
      "Epoch 62/100\n",
      "786/786 [==============================] - 0s 268us/step - loss: 1.9982 - acc: 0.1781\n",
      "Epoch 63/100\n",
      "786/786 [==============================] - 0s 326us/step - loss: 2.0022 - acc: 0.1845\n",
      "Epoch 64/100\n",
      "786/786 [==============================] - 0s 217us/step - loss: 1.9957 - acc: 0.1896\n",
      "Epoch 65/100\n",
      "786/786 [==============================] - 0s 280us/step - loss: 1.9968 - acc: 0.1921\n",
      "Epoch 66/100\n",
      "786/786 [==============================] - 0s 259us/step - loss: 2.0028 - acc: 0.1883\n",
      "Epoch 67/100\n",
      "786/786 [==============================] - 0s 257us/step - loss: 1.9965 - acc: 0.1832\n",
      "Epoch 68/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 1.9960 - acc: 0.1908\n",
      "Epoch 69/100\n",
      "786/786 [==============================] - 0s 264us/step - loss: 1.9990 - acc: 0.1819\n",
      "Epoch 70/100\n",
      "786/786 [==============================] - 0s 261us/step - loss: 1.9978 - acc: 0.1947\n",
      "Epoch 71/100\n",
      "786/786 [==============================] - 0s 254us/step - loss: 2.0020 - acc: 0.1743\n",
      "Epoch 72/100\n",
      "786/786 [==============================] - 0s 256us/step - loss: 2.0027 - acc: 0.1921\n",
      "Epoch 73/100\n",
      "786/786 [==============================] - 0s 203us/step - loss: 1.9956 - acc: 0.1794\n",
      "Epoch 74/100\n",
      "786/786 [==============================] - 0s 239us/step - loss: 1.9995 - acc: 0.1807\n",
      "Epoch 75/100\n",
      "786/786 [==============================] - 0s 183us/step - loss: 1.9983 - acc: 0.1845\n",
      "Epoch 76/100\n",
      "786/786 [==============================] - 0s 187us/step - loss: 1.9972 - acc: 0.1883\n",
      "Epoch 77/100\n",
      "786/786 [==============================] - 0s 240us/step - loss: 1.9997 - acc: 0.1870\n",
      "Epoch 78/100\n",
      "786/786 [==============================] - 0s 244us/step - loss: 1.9896 - acc: 0.1819\n",
      "Epoch 79/100\n",
      "786/786 [==============================] - 0s 225us/step - loss: 1.9959 - acc: 0.1819\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786/786 [==============================] - 0s 271us/step - loss: 1.9983 - acc: 0.1768\n",
      "Epoch 81/100\n",
      "786/786 [==============================] - 0s 164us/step - loss: 1.9949 - acc: 0.1896\n",
      "Epoch 82/100\n",
      "786/786 [==============================] - 0s 207us/step - loss: 1.9982 - acc: 0.1781\n",
      "Epoch 83/100\n",
      "786/786 [==============================] - 0s 214us/step - loss: 1.9912 - acc: 0.2023\n",
      "Epoch 84/100\n",
      "786/786 [==============================] - 0s 212us/step - loss: 1.9988 - acc: 0.1832\n",
      "Epoch 85/100\n",
      "786/786 [==============================] - 0s 235us/step - loss: 1.9959 - acc: 0.1959\n",
      "Epoch 86/100\n",
      "786/786 [==============================] - 0s 353us/step - loss: 1.9907 - acc: 0.1832\n",
      "Epoch 87/100\n",
      "786/786 [==============================] - 0s 247us/step - loss: 1.9935 - acc: 0.1908\n",
      "Epoch 88/100\n",
      "786/786 [==============================] - 0s 187us/step - loss: 1.9836 - acc: 0.1972\n",
      "Epoch 89/100\n",
      "786/786 [==============================] - 0s 170us/step - loss: 1.9926 - acc: 0.1908\n",
      "Epoch 90/100\n",
      "786/786 [==============================] - 0s 200us/step - loss: 1.9990 - acc: 0.1832\n",
      "Epoch 91/100\n",
      "786/786 [==============================] - 0s 169us/step - loss: 1.9934 - acc: 0.1870\n",
      "Epoch 92/100\n",
      "786/786 [==============================] - 0s 327us/step - loss: 1.9929 - acc: 0.1768\n",
      "Epoch 93/100\n",
      "786/786 [==============================] - 0s 197us/step - loss: 1.9993 - acc: 0.1883\n",
      "Epoch 94/100\n",
      "786/786 [==============================] - 0s 175us/step - loss: 1.9932 - acc: 0.1794\n",
      "Epoch 95/100\n",
      "786/786 [==============================] - 0s 184us/step - loss: 1.9909 - acc: 0.1858\n",
      "Epoch 96/100\n",
      "786/786 [==============================] - 0s 189us/step - loss: 1.9914 - acc: 0.1794\n",
      "Epoch 97/100\n",
      "786/786 [==============================] - 0s 172us/step - loss: 1.9948 - acc: 0.1743\n",
      "Epoch 98/100\n",
      "786/786 [==============================] - 0s 201us/step - loss: 1.9954 - acc: 0.1718\n",
      "Epoch 99/100\n",
      "786/786 [==============================] - 0s 214us/step - loss: 2.0003 - acc: 0.1705\n",
      "Epoch 100/100\n",
      "786/786 [==============================] - 0s 198us/step - loss: 1.9918 - acc: 0.2036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c444148fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train = X_train.reshape((392, 1, 50))\n",
    "\n",
    "# fit network\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "# evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 0s 2ms/step\n",
      "[2.0082246034883604, 0.14213197969543148]\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of peaks model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 1, 1)\n",
      "(786, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 1, 64)             128       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 41,672\n",
      "Trainable params: 41,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "786/786 [==============================] - 2s 3ms/step - loss: 2.0839 - acc: 0.1361\n",
      "Epoch 2/100\n",
      "786/786 [==============================] - 0s 252us/step - loss: 2.0847 - acc: 0.1272\n",
      "Epoch 3/100\n",
      "786/786 [==============================] - 0s 235us/step - loss: 2.0856 - acc: 0.1069\n",
      "Epoch 4/100\n",
      "786/786 [==============================] - 0s 235us/step - loss: 2.0825 - acc: 0.1234\n",
      "Epoch 5/100\n",
      "786/786 [==============================] - 0s 210us/step - loss: 2.0807 - acc: 0.1170\n",
      "Epoch 6/100\n",
      "786/786 [==============================] - 0s 250us/step - loss: 2.0803 - acc: 0.1272\n",
      "Epoch 7/100\n",
      "786/786 [==============================] - 0s 458us/step - loss: 2.0797 - acc: 0.1412\n",
      "Epoch 8/100\n",
      "786/786 [==============================] - 0s 367us/step - loss: 2.0836 - acc: 0.1120\n",
      "Epoch 9/100\n",
      "786/786 [==============================] - 0s 317us/step - loss: 2.0841 - acc: 0.1132\n",
      "Epoch 10/100\n",
      "786/786 [==============================] - 0s 585us/step - loss: 2.0785 - acc: 0.1438\n",
      "Epoch 11/100\n",
      "786/786 [==============================] - 0s 497us/step - loss: 2.0810 - acc: 0.1145\n",
      "Epoch 12/100\n",
      "786/786 [==============================] - 0s 290us/step - loss: 2.0803 - acc: 0.1476\n",
      "Epoch 13/100\n",
      "786/786 [==============================] - 0s 254us/step - loss: 2.0791 - acc: 0.1247\n",
      "Epoch 14/100\n",
      "786/786 [==============================] - 0s 448us/step - loss: 2.0812 - acc: 0.1260\n",
      "Epoch 15/100\n",
      "786/786 [==============================] - 0s 334us/step - loss: 2.0776 - acc: 0.1349 0s - loss: 2.0753 - acc: 0.13\n",
      "Epoch 16/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 2.0799 - acc: 0.1336\n",
      "Epoch 17/100\n",
      "786/786 [==============================] - 0s 340us/step - loss: 2.0804 - acc: 0.1374\n",
      "Epoch 18/100\n",
      "786/786 [==============================] - 0s 570us/step - loss: 2.0806 - acc: 0.1374\n",
      "Epoch 19/100\n",
      "786/786 [==============================] - 0s 366us/step - loss: 2.0790 - acc: 0.1374\n",
      "Epoch 20/100\n",
      "786/786 [==============================] - 0s 434us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 21/100\n",
      "786/786 [==============================] - 0s 336us/step - loss: 2.0790 - acc: 0.1374\n",
      "Epoch 22/100\n",
      "786/786 [==============================] - 0s 273us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 23/100\n",
      "786/786 [==============================] - 0s 256us/step - loss: 2.0786 - acc: 0.1374\n",
      "Epoch 24/100\n",
      "786/786 [==============================] - 0s 334us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 25/100\n",
      "786/786 [==============================] - 0s 567us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 26/100\n",
      "786/786 [==============================] - 0s 353us/step - loss: 2.0786 - acc: 0.1374\n",
      "Epoch 27/100\n",
      "786/786 [==============================] - 0s 420us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 28/100\n",
      "786/786 [==============================] - 0s 340us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 29/100\n",
      "786/786 [==============================] - 0s 562us/step - loss: 2.0779 - acc: 0.1374 0s - loss: 2.0803 - acc: 0.1\n",
      "Epoch 30/100\n",
      "786/786 [==============================] - 0s 469us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 31/100\n",
      "786/786 [==============================] - 0s 366us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 32/100\n",
      "786/786 [==============================] - 0s 577us/step - loss: 2.0785 - acc: 0.1374\n",
      "Epoch 33/100\n",
      "786/786 [==============================] - 0s 312us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 34/100\n",
      "786/786 [==============================] - 0s 270us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 35/100\n",
      "786/786 [==============================] - 0s 502us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 36/100\n",
      "786/786 [==============================] - 0s 483us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 37/100\n",
      "786/786 [==============================] - 0s 359us/step - loss: 2.0784 - acc: 0.1374\n",
      "Epoch 38/100\n",
      "786/786 [==============================] - 0s 378us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 39/100\n",
      "786/786 [==============================] - 0s 507us/step - loss: 2.0777 - acc: 0.1374\n",
      "Epoch 40/100\n",
      "786/786 [==============================] - 0s 331us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 41/100\n",
      "786/786 [==============================] - 0s 388us/step - loss: 2.0786 - acc: 0.1374\n",
      "Epoch 42/100\n",
      "786/786 [==============================] - 0s 482us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 43/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 44/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 45/100\n",
      "786/786 [==============================] - 0s 526us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 46/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 47/100\n",
      "786/786 [==============================] - 0s 322us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 48/100\n",
      "786/786 [==============================] - 0s 315us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 49/100\n",
      "786/786 [==============================] - 0s 518us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 50/100\n",
      "786/786 [==============================] - 0s 284us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 51/100\n",
      "786/786 [==============================] - 0s 308us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 52/100\n",
      "786/786 [==============================] - 0s 276us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 53/100\n",
      "786/786 [==============================] - 0s 521us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 54/100\n",
      "786/786 [==============================] - 0s 356us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 55/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 56/100\n",
      "786/786 [==============================] - 0s 278us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 57/100\n",
      "786/786 [==============================] - 0s 425us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 58/100\n",
      "786/786 [==============================] - 0s 327us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 59/100\n",
      "786/786 [==============================] - 0s 472us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 60/100\n",
      "786/786 [==============================] - 0s 392us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 61/100\n",
      "786/786 [==============================] - 0s 300us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 62/100\n",
      "786/786 [==============================] - 0s 253us/step - loss: 2.0781 - acc: 0.1374 0s - loss: 2.0768 - acc: 0.1\n",
      "Epoch 63/100\n",
      "786/786 [==============================] - 0s 364us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 64/100\n",
      "786/786 [==============================] - 0s 605us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 65/100\n",
      "786/786 [==============================] - 0s 357us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 66/100\n",
      "786/786 [==============================] - 0s 300us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 67/100\n",
      "786/786 [==============================] - 0s 343us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 68/100\n",
      "786/786 [==============================] - 0s 588us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 69/100\n",
      "786/786 [==============================] - 0s 257us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786/786 [==============================] - 0s 295us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 71/100\n",
      "786/786 [==============================] - 0s 397us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 72/100\n",
      "786/786 [==============================] - 0s 413us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 73/100\n",
      "786/786 [==============================] - 0s 329us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 74/100\n",
      "786/786 [==============================] - 0s 277us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 75/100\n",
      "786/786 [==============================] - 0s 488us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 76/100\n",
      "786/786 [==============================] - 0s 417us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 77/100\n",
      "786/786 [==============================] - 0s 284us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 78/100\n",
      "786/786 [==============================] - 0s 200us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 79/100\n",
      "786/786 [==============================] - 0s 576us/step - loss: 2.0782 - acc: 0.1374\n",
      "Epoch 80/100\n",
      "786/786 [==============================] - 0s 385us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 81/100\n",
      "786/786 [==============================] - 0s 365us/step - loss: 2.0780 - acc: 0.1374 0s - loss: 2.0799 - acc: 0.12\n",
      "Epoch 82/100\n",
      "786/786 [==============================] - 0s 429us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 83/100\n",
      "786/786 [==============================] - 0s 315us/step - loss: 2.0784 - acc: 0.1374\n",
      "Epoch 84/100\n",
      "786/786 [==============================] - 0s 347us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 85/100\n",
      "786/786 [==============================] - 0s 295us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 86/100\n",
      "786/786 [==============================] - 0s 262us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 87/100\n",
      "786/786 [==============================] - 0s 456us/step - loss: 2.0783 - acc: 0.1374\n",
      "Epoch 88/100\n",
      "786/786 [==============================] - 0s 317us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 89/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 2.0780 - acc: 0.1374 0s - loss: 2.0768 - acc: 0.14\n",
      "Epoch 90/100\n",
      "786/786 [==============================] - 0s 305us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 91/100\n",
      "786/786 [==============================] - 0s 536us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 92/100\n",
      "786/786 [==============================] - 0s 353us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 93/100\n",
      "786/786 [==============================] - 0s 320us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 94/100\n",
      "786/786 [==============================] - 0s 450us/step - loss: 2.0781 - acc: 0.1374\n",
      "Epoch 95/100\n",
      "786/786 [==============================] - 0s 338us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 96/100\n",
      "786/786 [==============================] - 0s 345us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 97/100\n",
      "786/786 [==============================] - 0s 444us/step - loss: 2.0779 - acc: 0.1374\n",
      "Epoch 98/100\n",
      "786/786 [==============================] - 0s 388us/step - loss: 2.0780 - acc: 0.1374\n",
      "Epoch 99/100\n",
      "786/786 [==============================] - 0s 404us/step - loss: 2.0778 - acc: 0.1374\n",
      "Epoch 100/100\n",
      "786/786 [==============================] - 0s 341us/step - loss: 2.0780 - acc: 0.1374\n",
      "197/197 [==============================] - 1s 3ms/step\n",
      "[2.095928105000917, 0.07614213197969544]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(numbofpeaks, to_categorical(labels), test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model_numofpeaks = Sequential()\n",
    "model_numofpeaks.add(Conv1D(filters=64, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model_numofpeaks.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model_numofpeaks.add(Dropout(0.5))\n",
    "model_numofpeaks.add(MaxPooling1D(pool_size=1))\n",
    "model_numofpeaks.add(Flatten())\n",
    "model_numofpeaks.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model_numofpeaks.add(Dense(n_outputs, activation='softmax'))\n",
    "model_numofpeaks.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_numofpeaks.summary()\n",
    "model_numofpeaks.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "accuracy = model_numofpeaks.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 1, 25)\n",
      "(786, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 1, 64)             1664      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 43,208\n",
      "Trainable params: 43,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "786/786 [==============================] - 2s 3ms/step - loss: 2.0110 - acc: 0.1476\n",
      "Epoch 2/100\n",
      "786/786 [==============================] - 0s 409us/step - loss: 1.9222 - acc: 0.1832\n",
      "Epoch 3/100\n",
      "786/786 [==============================] - 0s 263us/step - loss: 1.8570 - acc: 0.2341\n",
      "Epoch 4/100\n",
      "786/786 [==============================] - 0s 234us/step - loss: 1.7705 - acc: 0.3117\n",
      "Epoch 5/100\n",
      "786/786 [==============================] - 0s 272us/step - loss: 1.6599 - acc: 0.3830\n",
      "Epoch 6/100\n",
      "786/786 [==============================] - 0s 258us/step - loss: 1.6000 - acc: 0.3957\n",
      "Epoch 7/100\n",
      "786/786 [==============================] - 0s 336us/step - loss: 1.5174 - acc: 0.4326\n",
      "Epoch 8/100\n",
      "786/786 [==============================] - 0s 265us/step - loss: 1.4309 - acc: 0.4618\n",
      "Epoch 9/100\n",
      "786/786 [==============================] - 0s 274us/step - loss: 1.4113 - acc: 0.4415\n",
      "Epoch 10/100\n",
      "786/786 [==============================] - 0s 236us/step - loss: 1.3806 - acc: 0.4796\n",
      "Epoch 11/100\n",
      "786/786 [==============================] - 0s 283us/step - loss: 1.3504 - acc: 0.4669\n",
      "Epoch 12/100\n",
      "786/786 [==============================] - 0s 315us/step - loss: 1.3031 - acc: 0.4911\n",
      "Epoch 13/100\n",
      "786/786 [==============================] - 0s 258us/step - loss: 1.2699 - acc: 0.4962\n",
      "Epoch 14/100\n",
      "786/786 [==============================] - 0s 259us/step - loss: 1.2527 - acc: 0.5013\n",
      "Epoch 15/100\n",
      "786/786 [==============================] - 0s 293us/step - loss: 1.2234 - acc: 0.5254\n",
      "Epoch 16/100\n",
      "786/786 [==============================] - 0s 231us/step - loss: 1.1884 - acc: 0.5522\n",
      "Epoch 17/100\n",
      "786/786 [==============================] - 0s 315us/step - loss: 1.1564 - acc: 0.5547\n",
      "Epoch 18/100\n",
      "786/786 [==============================] - 0s 262us/step - loss: 1.1254 - acc: 0.5585\n",
      "Epoch 19/100\n",
      "786/786 [==============================] - 0s 273us/step - loss: 1.1364 - acc: 0.5623\n",
      "Epoch 20/100\n",
      "786/786 [==============================] - 0s 250us/step - loss: 1.1147 - acc: 0.5674\n",
      "Epoch 21/100\n",
      "786/786 [==============================] - 0s 353us/step - loss: 1.0549 - acc: 0.5929\n",
      "Epoch 22/100\n",
      "786/786 [==============================] - 0s 261us/step - loss: 1.0630 - acc: 0.5649\n",
      "Epoch 23/100\n",
      "786/786 [==============================] - 0s 267us/step - loss: 1.0575 - acc: 0.5827\n",
      "Epoch 24/100\n",
      "786/786 [==============================] - 0s 241us/step - loss: 1.0326 - acc: 0.5929\n",
      "Epoch 25/100\n",
      "786/786 [==============================] - 0s 300us/step - loss: 1.0320 - acc: 0.5840\n",
      "Epoch 26/100\n",
      "786/786 [==============================] - 0s 421us/step - loss: 1.0013 - acc: 0.6183\n",
      "Epoch 27/100\n",
      "786/786 [==============================] - 0s 228us/step - loss: 1.0122 - acc: 0.6069\n",
      "Epoch 28/100\n",
      "786/786 [==============================] - 0s 284us/step - loss: 0.9704 - acc: 0.6209\n",
      "Epoch 29/100\n",
      "786/786 [==============================] - 0s 264us/step - loss: 0.9536 - acc: 0.6438\n",
      "Epoch 30/100\n",
      "786/786 [==============================] - 0s 250us/step - loss: 0.9320 - acc: 0.6450\n",
      "Epoch 31/100\n",
      "786/786 [==============================] - 0s 337us/step - loss: 0.9498 - acc: 0.6234\n",
      "Epoch 32/100\n",
      "786/786 [==============================] - 0s 285us/step - loss: 0.9066 - acc: 0.6578\n",
      "Epoch 33/100\n",
      "786/786 [==============================] - 0s 271us/step - loss: 0.9172 - acc: 0.6374\n",
      "Epoch 34/100\n",
      "786/786 [==============================] - 0s 231us/step - loss: 0.8861 - acc: 0.6539\n",
      "Epoch 35/100\n",
      "786/786 [==============================] - 0s 258us/step - loss: 0.8796 - acc: 0.6552\n",
      "Epoch 36/100\n",
      "786/786 [==============================] - 0s 347us/step - loss: 0.8766 - acc: 0.6501\n",
      "Epoch 37/100\n",
      "786/786 [==============================] - 0s 254us/step - loss: 0.8438 - acc: 0.6730 0s - loss: 0.8555 - acc: 0.671\n",
      "Epoch 38/100\n",
      "786/786 [==============================] - 0s 343us/step - loss: 0.7893 - acc: 0.7112\n",
      "Epoch 39/100\n",
      "786/786 [==============================] - 0s 252us/step - loss: 0.8452 - acc: 0.6730\n",
      "Epoch 40/100\n",
      "786/786 [==============================] - 0s 257us/step - loss: 0.8355 - acc: 0.6819\n",
      "Epoch 41/100\n",
      "786/786 [==============================] - 0s 329us/step - loss: 0.8017 - acc: 0.6832\n",
      "Epoch 42/100\n",
      "786/786 [==============================] - 0s 335us/step - loss: 0.8151 - acc: 0.6692\n",
      "Epoch 43/100\n",
      "786/786 [==============================] - 0s 313us/step - loss: 0.7888 - acc: 0.7010\n",
      "Epoch 44/100\n",
      "786/786 [==============================] - 0s 319us/step - loss: 0.7596 - acc: 0.7061\n",
      "Epoch 45/100\n",
      "786/786 [==============================] - 0s 367us/step - loss: 0.7746 - acc: 0.7074\n",
      "Epoch 46/100\n",
      "786/786 [==============================] - 0s 305us/step - loss: 0.7314 - acc: 0.7023\n",
      "Epoch 47/100\n",
      "786/786 [==============================] - 0s 327us/step - loss: 0.7235 - acc: 0.7201\n",
      "Epoch 48/100\n",
      "786/786 [==============================] - 0s 330us/step - loss: 0.7322 - acc: 0.7036\n",
      "Epoch 49/100\n",
      "786/786 [==============================] - 0s 407us/step - loss: 0.7157 - acc: 0.7188\n",
      "Epoch 50/100\n",
      "786/786 [==============================] - 0s 334us/step - loss: 0.7372 - acc: 0.7112\n",
      "Epoch 51/100\n",
      "786/786 [==============================] - 0s 304us/step - loss: 0.7052 - acc: 0.7405\n",
      "Epoch 52/100\n",
      "786/786 [==============================] - 0s 314us/step - loss: 0.7014 - acc: 0.7354\n",
      "Epoch 53/100\n",
      "786/786 [==============================] - 0s 459us/step - loss: 0.6691 - acc: 0.7468\n",
      "Epoch 54/100\n",
      "786/786 [==============================] - 0s 336us/step - loss: 0.6556 - acc: 0.7519\n",
      "Epoch 55/100\n",
      "786/786 [==============================] - 0s 313us/step - loss: 0.6771 - acc: 0.7328\n",
      "Epoch 56/100\n",
      "786/786 [==============================] - 0s 323us/step - loss: 0.6875 - acc: 0.7455\n",
      "Epoch 57/100\n",
      "786/786 [==============================] - 0s 484us/step - loss: 0.6122 - acc: 0.7595\n",
      "Epoch 58/100\n",
      "786/786 [==============================] - 0s 340us/step - loss: 0.6258 - acc: 0.7506\n",
      "Epoch 59/100\n",
      "786/786 [==============================] - 0s 366us/step - loss: 0.6236 - acc: 0.7761\n",
      "Epoch 60/100\n",
      "786/786 [==============================] - 0s 579us/step - loss: 0.6500 - acc: 0.7519\n",
      "Epoch 61/100\n",
      "786/786 [==============================] - 0s 537us/step - loss: 0.5768 - acc: 0.7774\n",
      "Epoch 62/100\n",
      "786/786 [==============================] - 0s 356us/step - loss: 0.6510 - acc: 0.7430\n",
      "Epoch 63/100\n",
      "786/786 [==============================] - 0s 494us/step - loss: 0.6244 - acc: 0.7570\n",
      "Epoch 64/100\n",
      "786/786 [==============================] - 0s 326us/step - loss: 0.6329 - acc: 0.7557\n",
      "Epoch 65/100\n",
      "786/786 [==============================] - 0s 345us/step - loss: 0.6204 - acc: 0.7621\n",
      "Epoch 66/100\n",
      "786/786 [==============================] - 0s 373us/step - loss: 0.5944 - acc: 0.7697\n",
      "Epoch 67/100\n",
      "786/786 [==============================] - 0s 454us/step - loss: 0.5557 - acc: 0.7901\n",
      "Epoch 68/100\n",
      "786/786 [==============================] - 0s 354us/step - loss: 0.5635 - acc: 0.7850\n",
      "Epoch 69/100\n",
      "786/786 [==============================] - 0s 432us/step - loss: 0.5386 - acc: 0.7812\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786/786 [==============================] - 0s 390us/step - loss: 0.5383 - acc: 0.7990\n",
      "Epoch 71/100\n",
      "786/786 [==============================] - 0s 416us/step - loss: 0.5390 - acc: 0.8066 0s - loss: 0.5387 - acc: 0.807 - ETA: 0s - loss: 0.5223 - acc: 0.81\n",
      "Epoch 72/100\n",
      "786/786 [==============================] - 0s 594us/step - loss: 0.5107 - acc: 0.8193\n",
      "Epoch 73/100\n",
      "786/786 [==============================] - 1s 698us/step - loss: 0.5421 - acc: 0.7926\n",
      "Epoch 74/100\n",
      "786/786 [==============================] - 0s 483us/step - loss: 0.5300 - acc: 0.7913\n",
      "Epoch 75/100\n",
      "786/786 [==============================] - 1s 702us/step - loss: 0.5155 - acc: 0.8053\n",
      "Epoch 76/100\n",
      "786/786 [==============================] - 0s 490us/step - loss: 0.5258 - acc: 0.8028\n",
      "Epoch 77/100\n",
      "786/786 [==============================] - 0s 374us/step - loss: 0.4686 - acc: 0.8232\n",
      "Epoch 78/100\n",
      "786/786 [==============================] - 1s 657us/step - loss: 0.4969 - acc: 0.8206\n",
      "Epoch 79/100\n",
      "786/786 [==============================] - 0s 328us/step - loss: 0.4847 - acc: 0.8168\n",
      "Epoch 80/100\n",
      "786/786 [==============================] - 0s 275us/step - loss: 0.5100 - acc: 0.8117\n",
      "Epoch 81/100\n",
      "786/786 [==============================] - 0s 201us/step - loss: 0.4729 - acc: 0.8257\n",
      "Epoch 82/100\n",
      "786/786 [==============================] - 0s 191us/step - loss: 0.5185 - acc: 0.8066\n",
      "Epoch 83/100\n",
      "786/786 [==============================] - 0s 341us/step - loss: 0.4527 - acc: 0.8308\n",
      "Epoch 84/100\n",
      "786/786 [==============================] - 0s 291us/step - loss: 0.4951 - acc: 0.8117\n",
      "Epoch 85/100\n",
      "786/786 [==============================] - ETA: 0s - loss: 0.4815 - acc: 0.812 - 0s 308us/step - loss: 0.4752 - acc: 0.8168\n",
      "Epoch 86/100\n",
      "786/786 [==============================] - 0s 254us/step - loss: 0.4572 - acc: 0.8270\n",
      "Epoch 87/100\n",
      "786/786 [==============================] - 0s 337us/step - loss: 0.4456 - acc: 0.8244\n",
      "Epoch 88/100\n",
      "786/786 [==============================] - 0s 398us/step - loss: 0.4232 - acc: 0.8422\n",
      "Epoch 89/100\n",
      "786/786 [==============================] - 0s 301us/step - loss: 0.4625 - acc: 0.8321\n",
      "Epoch 90/100\n",
      "786/786 [==============================] - 0s 239us/step - loss: 0.4757 - acc: 0.8142\n",
      "Epoch 91/100\n",
      "786/786 [==============================] - 0s 308us/step - loss: 0.4487 - acc: 0.8232\n",
      "Epoch 92/100\n",
      "786/786 [==============================] - 0s 364us/step - loss: 0.4479 - acc: 0.8168\n",
      "Epoch 93/100\n",
      "786/786 [==============================] - 0s 277us/step - loss: 0.4327 - acc: 0.8384\n",
      "Epoch 94/100\n",
      "786/786 [==============================] - 0s 298us/step - loss: 0.3796 - acc: 0.8702\n",
      "Epoch 95/100\n",
      "786/786 [==============================] - 0s 425us/step - loss: 0.4093 - acc: 0.8473\n",
      "Epoch 96/100\n",
      "786/786 [==============================] - 0s 500us/step - loss: 0.4008 - acc: 0.8639\n",
      "Epoch 97/100\n",
      "786/786 [==============================] - 0s 378us/step - loss: 0.4030 - acc: 0.8282\n",
      "Epoch 98/100\n",
      "786/786 [==============================] - 0s 455us/step - loss: 0.3800 - acc: 0.8550\n",
      "Epoch 99/100\n",
      "786/786 [==============================] - 0s 425us/step - loss: 0.3935 - acc: 0.8473\n",
      "Epoch 100/100\n",
      "786/786 [==============================] - 0s 292us/step - loss: 0.3874 - acc: 0.8448\n",
      "197/197 [==============================] - 1s 3ms/step\n",
      "[1.6504714331651098, 0.5228426397451894]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, to_categorical(labels), test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model_data = Sequential()\n",
    "model_data.add(Conv1D(filters=64, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model_data.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model_data.add(Dropout(0.5))\n",
    "model_data.add(MaxPooling1D(pool_size=1))\n",
    "model_data.add(Flatten())\n",
    "model_data.add(Dense(512, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model_data.add(Dense(n_outputs, activation='softmax'))\n",
    "model_data.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_data.summary()\n",
    "model_data.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "accuracy = model_data.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another try with scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def scale_data(trainX, testX, standardize):\n",
    "\t# remove overlap\n",
    "\tcut = int(trainX.shape[1] / 2)\n",
    "\tlongX = trainX[:, -cut:, :]\n",
    "\t# flatten windows\n",
    "\tlongX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
    "\t# flatten train and test\n",
    "\tflatTrainX = trainX.reshape((trainX.shape[0] * trainX.shape[1], trainX.shape[2]))\n",
    "\tflatTestX = testX.reshape((testX.shape[0] * testX.shape[1], testX.shape[2]))\n",
    "\t# standardize\n",
    "\tif standardize:\n",
    "\t\ts = StandardScaler()\n",
    "\t\t# fit on training data\n",
    "\t\ts.fit(longX)\n",
    "\t\t# apply to training and test data\n",
    "\t\tlongX = s.transform(longX)\n",
    "\t\tflatTrainX = s.transform(flatTrainX)\n",
    "\t\tflatTestX = s.transform(flatTestX)\n",
    "\t# reshape\n",
    "\tflatTrainX = flatTrainX.reshape((trainX.shape))\n",
    "\tflatTestX = flatTestX.reshape((testX.shape))\n",
    "\treturn flatTrainX, flatTestX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX = scale_data(X_train, X_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose, epochs, batch_size = 0, 10, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 8\n",
    "model1 = Sequential()\n",
    "model1.add(Conv1D(filters=128, kernel_size=1, activation='relu', \n",
    "                   input_shape=(n_timesteps,n_features)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(MaxPooling1D(pool_size=1))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "# model1.add(Dense(64, activation='relu'))\n",
    "# model1.add(BatchNormalization())\n",
    "model1.add(Dense(n_outputs, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 1, 128)            3328      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 64)             256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 17,288\n",
      "Trainable params: 16,776\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "786/786 [==============================] - 4s 5ms/step - loss: 2.4588 - acc: 0.1489\n",
      "Epoch 2/150\n",
      "786/786 [==============================] - 0s 271us/step - loss: 2.1277 - acc: 0.2226\n",
      "Epoch 3/150\n",
      "786/786 [==============================] - 0s 364us/step - loss: 1.9520 - acc: 0.2646\n",
      "Epoch 4/150\n",
      "786/786 [==============================] - 0s 385us/step - loss: 1.8318 - acc: 0.3003\n",
      "Epoch 5/150\n",
      "786/786 [==============================] - 0s 337us/step - loss: 1.7394 - acc: 0.3435\n",
      "Epoch 6/150\n",
      "786/786 [==============================] - 0s 277us/step - loss: 1.7094 - acc: 0.3244\n",
      "Epoch 7/150\n",
      "786/786 [==============================] - 0s 293us/step - loss: 1.6479 - acc: 0.3588\n",
      "Epoch 8/150\n",
      "786/786 [==============================] - 0s 290us/step - loss: 1.6139 - acc: 0.3639\n",
      "Epoch 9/150\n",
      "786/786 [==============================] - 0s 338us/step - loss: 1.5748 - acc: 0.3842\n",
      "Epoch 10/150\n",
      "786/786 [==============================] - 0s 271us/step - loss: 1.5544 - acc: 0.3944\n",
      "Epoch 11/150\n",
      "786/786 [==============================] - 0s 373us/step - loss: 1.4890 - acc: 0.4160\n",
      "Epoch 12/150\n",
      "786/786 [==============================] - 0s 301us/step - loss: 1.5140 - acc: 0.4122\n",
      "Epoch 13/150\n",
      "786/786 [==============================] - 0s 392us/step - loss: 1.4935 - acc: 0.4198\n",
      "Epoch 14/150\n",
      "786/786 [==============================] - 0s 317us/step - loss: 1.5152 - acc: 0.4275\n",
      "Epoch 15/150\n",
      "786/786 [==============================] - 0s 403us/step - loss: 1.4235 - acc: 0.4580\n",
      "Epoch 16/150\n",
      "786/786 [==============================] - 0s 308us/step - loss: 1.4016 - acc: 0.4466\n",
      "Epoch 17/150\n",
      "786/786 [==============================] - 0s 473us/step - loss: 1.3883 - acc: 0.4529\n",
      "Epoch 18/150\n",
      "786/786 [==============================] - 0s 310us/step - loss: 1.3860 - acc: 0.4606\n",
      "Epoch 19/150\n",
      "786/786 [==============================] - 0s 290us/step - loss: 1.3830 - acc: 0.4771\n",
      "Epoch 20/150\n",
      "786/786 [==============================] - 0s 267us/step - loss: 1.3806 - acc: 0.4656\n",
      "Epoch 21/150\n",
      "786/786 [==============================] - 0s 259us/step - loss: 1.3706 - acc: 0.4682\n",
      "Epoch 22/150\n",
      "786/786 [==============================] - 0s 336us/step - loss: 1.2923 - acc: 0.5038\n",
      "Epoch 23/150\n",
      "786/786 [==============================] - 0s 277us/step - loss: 1.2693 - acc: 0.4771\n",
      "Epoch 24/150\n",
      "786/786 [==============================] - 0s 304us/step - loss: 1.2484 - acc: 0.5038\n",
      "Epoch 25/150\n",
      "786/786 [==============================] - 0s 272us/step - loss: 1.2708 - acc: 0.5127\n",
      "Epoch 26/150\n",
      "786/786 [==============================] - 0s 374us/step - loss: 1.2669 - acc: 0.4987\n",
      "Epoch 27/150\n",
      "786/786 [==============================] - 0s 284us/step - loss: 1.2082 - acc: 0.5394\n",
      "Epoch 28/150\n",
      "786/786 [==============================] - 0s 277us/step - loss: 1.2356 - acc: 0.5318\n",
      "Epoch 29/150\n",
      "786/786 [==============================] - 0s 265us/step - loss: 1.2266 - acc: 0.5331\n",
      "Epoch 30/150\n",
      "786/786 [==============================] - 0s 300us/step - loss: 1.1635 - acc: 0.5420\n",
      "Epoch 31/150\n",
      "786/786 [==============================] - 0s 418us/step - loss: 1.1708 - acc: 0.5471\n",
      "Epoch 32/150\n",
      "786/786 [==============================] - ETA: 0s - loss: 1.1498 - acc: 0.556 - 0s 389us/step - loss: 1.1544 - acc: 0.5534\n",
      "Epoch 33/150\n",
      "786/786 [==============================] - 0s 343us/step - loss: 1.1242 - acc: 0.5585\n",
      "Epoch 34/150\n",
      "786/786 [==============================] - 0s 353us/step - loss: 1.1715 - acc: 0.5560 0s - loss: 1.1820 - acc: 0.56\n",
      "Epoch 35/150\n",
      "786/786 [==============================] - 0s 516us/step - loss: 1.1427 - acc: 0.5547\n",
      "Epoch 36/150\n",
      "786/786 [==============================] - 0s 287us/step - loss: 1.1015 - acc: 0.5738\n",
      "Epoch 37/150\n",
      "786/786 [==============================] - 0s 315us/step - loss: 1.1295 - acc: 0.5522\n",
      "Epoch 38/150\n",
      "786/786 [==============================] - 0s 443us/step - loss: 1.1253 - acc: 0.5509\n",
      "Epoch 39/150\n",
      "786/786 [==============================] - 0s 563us/step - loss: 1.0930 - acc: 0.5840\n",
      "Epoch 40/150\n",
      "786/786 [==============================] - 0s 327us/step - loss: 1.0470 - acc: 0.6221\n",
      "Epoch 41/150\n",
      "786/786 [==============================] - 0s 388us/step - loss: 1.0457 - acc: 0.5916\n",
      "Epoch 42/150\n",
      "786/786 [==============================] - 0s 379us/step - loss: 1.0964 - acc: 0.5598\n",
      "Epoch 43/150\n",
      "786/786 [==============================] - 0s 292us/step - loss: 1.0358 - acc: 0.6043\n",
      "Epoch 44/150\n",
      "786/786 [==============================] - 0s 340us/step - loss: 1.0502 - acc: 0.5763\n",
      "Epoch 45/150\n",
      "786/786 [==============================] - 0s 437us/step - loss: 0.9860 - acc: 0.6081\n",
      "Epoch 46/150\n",
      "786/786 [==============================] - 0s 532us/step - loss: 1.0073 - acc: 0.6310\n",
      "Epoch 47/150\n",
      "786/786 [==============================] - 0s 322us/step - loss: 1.0713 - acc: 0.6031\n",
      "Epoch 48/150\n",
      "786/786 [==============================] - 0s 503us/step - loss: 0.9979 - acc: 0.6349\n",
      "Epoch 49/150\n",
      "786/786 [==============================] - 0s 559us/step - loss: 1.0062 - acc: 0.6145\n",
      "Epoch 50/150\n",
      "786/786 [==============================] - 0s 319us/step - loss: 1.0140 - acc: 0.6247\n",
      "Epoch 51/150\n",
      "786/786 [==============================] - 0s 506us/step - loss: 0.9765 - acc: 0.6145\n",
      "Epoch 52/150\n",
      "786/786 [==============================] - 0s 319us/step - loss: 0.9124 - acc: 0.6603\n",
      "Epoch 53/150\n",
      "786/786 [==============================] - 0s 459us/step - loss: 0.9546 - acc: 0.6323\n",
      "Epoch 54/150\n",
      "786/786 [==============================] - 0s 347us/step - loss: 0.9301 - acc: 0.6336\n",
      "Epoch 55/150\n",
      "786/786 [==============================] - 0s 326us/step - loss: 0.9507 - acc: 0.6285\n",
      "Epoch 56/150\n",
      "786/786 [==============================] - 0s 440us/step - loss: 0.9525 - acc: 0.6196\n",
      "Epoch 57/150\n",
      "786/786 [==============================] - 0s 341us/step - loss: 0.9462 - acc: 0.6323\n",
      "Epoch 58/150\n",
      "786/786 [==============================] - 0s 345us/step - loss: 0.9785 - acc: 0.6336\n",
      "Epoch 59/150\n",
      "786/786 [==============================] - 0s 341us/step - loss: 0.9335 - acc: 0.6514\n",
      "Epoch 60/150\n",
      "786/786 [==============================] - 0s 409us/step - loss: 0.8872 - acc: 0.6641\n",
      "Epoch 61/150\n",
      "786/786 [==============================] - 0s 392us/step - loss: 0.8998 - acc: 0.6578\n",
      "Epoch 62/150\n",
      "786/786 [==============================] - 0s 264us/step - loss: 0.8675 - acc: 0.6705\n",
      "Epoch 63/150\n",
      "786/786 [==============================] - 0s 310us/step - loss: 0.9346 - acc: 0.6438\n",
      "Epoch 64/150\n",
      "786/786 [==============================] - 0s 430us/step - loss: 0.8830 - acc: 0.6501 0s - loss: 0.7946 - acc: 0.\n",
      "Epoch 65/150\n",
      "786/786 [==============================] - 0s 329us/step - loss: 0.8673 - acc: 0.6743\n",
      "Epoch 66/150\n",
      "786/786 [==============================] - 0s 314us/step - loss: 0.9181 - acc: 0.6425\n",
      "Epoch 67/150\n",
      "786/786 [==============================] - 0s 310us/step - loss: 0.9251 - acc: 0.6450\n",
      "Epoch 68/150\n",
      "786/786 [==============================] - 0s 343us/step - loss: 0.8618 - acc: 0.6539\n",
      "Epoch 69/150\n",
      "786/786 [==============================] - 0s 258us/step - loss: 0.8578 - acc: 0.6539\n",
      "Epoch 70/150\n",
      "786/786 [==============================] - 0s 281us/step - loss: 0.9052 - acc: 0.6387\n",
      "Epoch 71/150\n",
      "786/786 [==============================] - 0s 235us/step - loss: 0.8381 - acc: 0.6845\n",
      "Epoch 72/150\n",
      "786/786 [==============================] - 0s 299us/step - loss: 0.8609 - acc: 0.6692\n",
      "Epoch 73/150\n",
      "786/786 [==============================] - 0s 322us/step - loss: 0.9046 - acc: 0.6361\n",
      "Epoch 74/150\n",
      "786/786 [==============================] - 0s 280us/step - loss: 0.8592 - acc: 0.6412\n",
      "Epoch 75/150\n",
      "786/786 [==============================] - 0s 249us/step - loss: 0.8481 - acc: 0.6819\n",
      "Epoch 76/150\n",
      "786/786 [==============================] - 0s 287us/step - loss: 0.8311 - acc: 0.6858\n",
      "Epoch 77/150\n",
      "786/786 [==============================] - 0s 259us/step - loss: 0.8587 - acc: 0.6628\n",
      "Epoch 78/150\n",
      "786/786 [==============================] - 0s 301us/step - loss: 0.8353 - acc: 0.6908\n",
      "Epoch 79/150\n",
      "786/786 [==============================] - 0s 262us/step - loss: 0.8293 - acc: 0.6743\n",
      "Epoch 80/150\n",
      "786/786 [==============================] - 0s 240us/step - loss: 0.8352 - acc: 0.6819\n",
      "Epoch 81/150\n",
      "786/786 [==============================] - 0s 258us/step - loss: 0.7638 - acc: 0.7125\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786/786 [==============================] - 0s 259us/step - loss: 0.7914 - acc: 0.6985\n",
      "Epoch 83/150\n",
      "786/786 [==============================] - 0s 268us/step - loss: 0.7629 - acc: 0.7099\n",
      "Epoch 84/150\n",
      "786/786 [==============================] - 0s 296us/step - loss: 0.7945 - acc: 0.6896\n",
      "Epoch 85/150\n",
      "786/786 [==============================] - 0s 233us/step - loss: 0.8203 - acc: 0.6832\n",
      "Epoch 86/150\n",
      "786/786 [==============================] - 0s 239us/step - loss: 0.7865 - acc: 0.7087\n",
      "Epoch 87/150\n",
      "786/786 [==============================] - 0s 271us/step - loss: 0.7941 - acc: 0.6947\n",
      "Epoch 88/150\n",
      "786/786 [==============================] - 0s 267us/step - loss: 0.7842 - acc: 0.6947\n",
      "Epoch 89/150\n",
      "786/786 [==============================] - 0s 364us/step - loss: 0.7641 - acc: 0.7290\n",
      "Epoch 90/150\n",
      "786/786 [==============================] - 0s 299us/step - loss: 0.8065 - acc: 0.6896\n",
      "Epoch 91/150\n",
      "786/786 [==============================] - 0s 299us/step - loss: 0.7148 - acc: 0.7265\n",
      "Epoch 92/150\n",
      "786/786 [==============================] - 0s 266us/step - loss: 0.7729 - acc: 0.6997\n",
      "Epoch 93/150\n",
      "786/786 [==============================] - 0s 299us/step - loss: 0.7328 - acc: 0.7150\n",
      "Epoch 94/150\n",
      "786/786 [==============================] - 0s 286us/step - loss: 0.7837 - acc: 0.6896\n",
      "Epoch 95/150\n",
      "786/786 [==============================] - 0s 318us/step - loss: 0.7920 - acc: 0.6896\n",
      "Epoch 96/150\n",
      "786/786 [==============================] - 0s 273us/step - loss: 0.7865 - acc: 0.7010\n",
      "Epoch 97/150\n",
      "786/786 [==============================] - 0s 266us/step - loss: 0.7286 - acc: 0.7137\n",
      "Epoch 98/150\n",
      "786/786 [==============================] - 0s 318us/step - loss: 0.7246 - acc: 0.7112\n",
      "Epoch 99/150\n",
      "786/786 [==============================] - 0s 273us/step - loss: 0.7267 - acc: 0.7328\n",
      "Epoch 100/150\n",
      "786/786 [==============================] - 0s 286us/step - loss: 0.7366 - acc: 0.7176\n",
      "Epoch 101/150\n",
      "786/786 [==============================] - 0s 291us/step - loss: 0.6882 - acc: 0.7290\n",
      "Epoch 102/150\n",
      "786/786 [==============================] - 0s 274us/step - loss: 0.7342 - acc: 0.7061\n",
      "Epoch 103/150\n",
      "786/786 [==============================] - 0s 331us/step - loss: 0.7312 - acc: 0.7239\n",
      "Epoch 104/150\n",
      "786/786 [==============================] - 0s 280us/step - loss: 0.7035 - acc: 0.7290\n",
      "Epoch 105/150\n",
      "786/786 [==============================] - 0s 263us/step - loss: 0.6794 - acc: 0.7494\n",
      "Epoch 106/150\n",
      "786/786 [==============================] - 0s 278us/step - loss: 0.6977 - acc: 0.7277\n",
      "Epoch 107/150\n",
      "786/786 [==============================] - 0s 284us/step - loss: 0.6556 - acc: 0.7506\n",
      "Epoch 108/150\n",
      "786/786 [==============================] - 0s 352us/step - loss: 0.7217 - acc: 0.7252\n",
      "Epoch 109/150\n",
      "786/786 [==============================] - 0s 280us/step - loss: 0.6671 - acc: 0.7201\n",
      "Epoch 110/150\n",
      "786/786 [==============================] - 0s 260us/step - loss: 0.7056 - acc: 0.7226\n",
      "Epoch 111/150\n",
      "786/786 [==============================] - 0s 277us/step - loss: 0.6870 - acc: 0.7328\n",
      "Epoch 112/150\n",
      "786/786 [==============================] - 0s 329us/step - loss: 0.6868 - acc: 0.7506\n",
      "Epoch 113/150\n",
      "786/786 [==============================] - 0s 287us/step - loss: 0.6314 - acc: 0.7646\n",
      "Epoch 114/150\n",
      "786/786 [==============================] - 0s 270us/step - loss: 0.6848 - acc: 0.7341\n",
      "Epoch 115/150\n",
      "786/786 [==============================] - 0s 284us/step - loss: 0.6771 - acc: 0.7277\n",
      "Epoch 116/150\n",
      "786/786 [==============================] - 0s 284us/step - loss: 0.7226 - acc: 0.7226\n",
      "Epoch 117/150\n",
      "786/786 [==============================] - 0s 309us/step - loss: 0.7057 - acc: 0.7265\n",
      "Epoch 118/150\n",
      "786/786 [==============================] - 0s 275us/step - loss: 0.6398 - acc: 0.7583\n",
      "Epoch 119/150\n",
      "786/786 [==============================] - 0s 259us/step - loss: 0.6515 - acc: 0.7697\n",
      "Epoch 120/150\n",
      "786/786 [==============================] - 0s 276us/step - loss: 0.6658 - acc: 0.7506\n",
      "Epoch 121/150\n",
      "786/786 [==============================] - 0s 334us/step - loss: 0.6546 - acc: 0.7595\n",
      "Epoch 122/150\n",
      "786/786 [==============================] - 0s 339us/step - loss: 0.6420 - acc: 0.7443\n",
      "Epoch 123/150\n",
      "786/786 [==============================] - 0s 324us/step - loss: 0.6619 - acc: 0.7468\n",
      "Epoch 124/150\n",
      "786/786 [==============================] - 0s 268us/step - loss: 0.6882 - acc: 0.7303\n",
      "Epoch 125/150\n",
      "786/786 [==============================] - 0s 292us/step - loss: 0.6308 - acc: 0.7570\n",
      "Epoch 126/150\n",
      "786/786 [==============================] - 0s 329us/step - loss: 0.6325 - acc: 0.7634\n",
      "Epoch 127/150\n",
      "786/786 [==============================] - 0s 276us/step - loss: 0.6378 - acc: 0.7443\n",
      "Epoch 128/150\n",
      "786/786 [==============================] - 0s 281us/step - loss: 0.6517 - acc: 0.7443\n",
      "Epoch 129/150\n",
      "786/786 [==============================] - 0s 345us/step - loss: 0.6112 - acc: 0.7672\n",
      "Epoch 130/150\n",
      "786/786 [==============================] - 0s 312us/step - loss: 0.7012 - acc: 0.7163\n",
      "Epoch 131/150\n",
      "786/786 [==============================] - 0s 352us/step - loss: 0.6617 - acc: 0.7481\n",
      "Epoch 132/150\n",
      "786/786 [==============================] - 0s 276us/step - loss: 0.6264 - acc: 0.7506\n",
      "Epoch 133/150\n",
      "786/786 [==============================] - 0s 331us/step - loss: 0.6684 - acc: 0.7354\n",
      "Epoch 134/150\n",
      "786/786 [==============================] - 0s 321us/step - loss: 0.6194 - acc: 0.7697\n",
      "Epoch 135/150\n",
      "786/786 [==============================] - 0s 356us/step - loss: 0.6020 - acc: 0.7812\n",
      "Epoch 136/150\n",
      "786/786 [==============================] - 0s 289us/step - loss: 0.6113 - acc: 0.7684\n",
      "Epoch 137/150\n",
      "786/786 [==============================] - 0s 291us/step - loss: 0.6042 - acc: 0.7545\n",
      "Epoch 138/150\n",
      "786/786 [==============================] - 0s 289us/step - loss: 0.6260 - acc: 0.7672\n",
      "Epoch 139/150\n",
      "786/786 [==============================] - 0s 282us/step - loss: 0.6611 - acc: 0.7570\n",
      "Epoch 140/150\n",
      "786/786 [==============================] - 0s 384us/step - loss: 0.5788 - acc: 0.7621\n",
      "Epoch 141/150\n",
      "786/786 [==============================] - 0s 296us/step - loss: 0.5910 - acc: 0.7672\n",
      "Epoch 142/150\n",
      "786/786 [==============================] - 0s 268us/step - loss: 0.6257 - acc: 0.7519\n",
      "Epoch 143/150\n",
      "786/786 [==============================] - 0s 292us/step - loss: 0.6745 - acc: 0.7354\n",
      "Epoch 144/150\n",
      "786/786 [==============================] - 0s 345us/step - loss: 0.6497 - acc: 0.7532\n",
      "Epoch 145/150\n",
      "786/786 [==============================] - 0s 301us/step - loss: 0.6138 - acc: 0.7646\n",
      "Epoch 146/150\n",
      "786/786 [==============================] - 0s 289us/step - loss: 0.6246 - acc: 0.7761\n",
      "Epoch 147/150\n",
      "786/786 [==============================] - 0s 281us/step - loss: 0.5899 - acc: 0.7799\n",
      "Epoch 148/150\n",
      "786/786 [==============================] - 0s 265us/step - loss: 0.6038 - acc: 0.7621\n",
      "Epoch 149/150\n",
      "786/786 [==============================] - 0s 310us/step - loss: 0.5828 - acc: 0.7748\n",
      "Epoch 150/150\n",
      "786/786 [==============================] - 0s 277us/step - loss: 0.5848 - acc: 0.7901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c43b3fcfd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(trainX, y_train, epochs=150, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 1s 5ms/step\n",
      "[1.6520405081928078, 0.4771573605573722]\n"
     ]
    }
   ],
   "source": [
    "accuracy = model1.evaluate(testX, y_test, batch_size=32, verbose=1)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* features extraction was not usefull as we have seen. we noticed from the data behaviour that the value of the peaks matters more than the number of peaks.(to be considered in further work)\n",
    "* ECG data was giving acuraccy of 58% at some moments which is better than the other trials. but still we will try to increase the accuracy in 2D CNN   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "* TuanNguyen GiaaImed Ben Dhaoub Mai Alic Amir M.Rahmanide Tomi Wester lunda Pasi Liljeberga Hannu Tenhunena. “Energy efficient fog-assisted IoT system for monitoring diabetic patients with cardiovascular disease”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
